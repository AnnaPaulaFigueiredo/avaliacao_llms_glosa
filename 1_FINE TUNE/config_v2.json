{
  "wandb_api_key": "",
  "hf_read_token": "",
  "hf_write_token": "",
  "project_name": "Chatbot-Finetune-UnslothV2",
  "output_base_dir": "./checkpoints_unsloth/v2",
    "models": [
             {
          "model_name": "unsloth/gemma-3-12b-it-bnb-4bit",
          "dataset_name": "annagoncalves2/glosa",
          "repo_name": "chatbot-gemma-3-12b-it-bnb-4bit-V2"
        },
        {
          "model_name": "unsloth/Llama-3.1-8B-unsloth-bnb-4bit",
          "dataset_name": "annagoncalves2/glosa",
          "repo_name": "chatbot-Llama-3.1-8B-unsloth-bnb-4bit-V2"
        },
        {
          "model_name": "unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit",
          "dataset_name": "annagoncalves2/glosa",
          "repo_name": "chatbot-Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V2"
        },
        {
          "model_name": "unsloth/phi-4-unsloth-bnb-4bit",
          "dataset_name": "annagoncalves2/glosa",
          "repo_name": "chatbot-phi-4-unsloth-bnb-4bit-V2"
        },
        {
          "model_name": "unsloth/Qwen2.5-7B-Instruct-bnb-4bit",
          "dataset_name": "annagoncalves2/glosa",
          "repo_name": "chatbot-Qwen2.5-7B-Instruct-bnb-4bit-V2"
        },
        {
          "model_name": "unsloth/Qwen2.5-14B-Instruct-unsloth-bnb-4bit",
          "dataset_name": "annagoncalves2/glosa",
          "repo_name": "chatbot-Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V2"
        },
        {
          "model_name": "unsloth/zephyr-sft-bnb-4bit",
          "dataset_name": "annagoncalves2/glosa",
          "repo_name": "chatbot-zephyr-sft-bnb-4bit-V2"
        },
        {
          "model_name": "unsloth/DeepSeek-R1-Distill-Llama-8B",
          "dataset_name": "annagoncalves2/glosa",
          "repo_name": "chatbot-DeepSeek-R1-Distill-Llama-8B-V2"
        }
    ],
    "training_args": {
      "per_device_train_batch_size": 4,
      "gradient_accumulation_steps": 8,
      "warmup_steps": 100,
      "num_train_epochs": 5,
      "learning_rate": 1.5e-4,
      "logging_steps": 5,
      "save_steps": 50,
      "save_total_limit": 3,
      "weight_decay": 0.02,
      "lr_scheduler_type": "cosine",
      "seed": 3407,
      "max_seq_length": 4096,
      "dataset_num_proc": 4,
      "packing": false,
      "optim": "adamw_8bit"
    },
    "lora_r": 32,
    "lora_target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "lora_bias": "none",
    "use_gradient_checkpointing": "unsloth",
    "random_state": 3407,
    "use_rslora": true,
    "loftq_config": null
  }
  