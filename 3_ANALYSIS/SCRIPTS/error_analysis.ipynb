{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5451db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3cd2486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando: Llama-3.2-3B-V1\n",
      "Processando: Phi-4-14B-V1\n",
      "Processando: Zephyr-7B-V1\n",
      "Processando: Llama-3.1-8B-V1\n",
      "Processando: Qwen2.5-7B-V1\n",
      "Processando: Qwen2.5-14B-V1\n"
     ]
    }
   ],
   "source": [
    "dir_path = '../UNSLOTH/V1/GLOSA_PT'\n",
    "answer_path = '/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv'\n",
    "\n",
    "models_map = {\n",
    "    'Llama-3.1-8B-unsloth-bnb-4bit-V1.csv': 'Llama-3.1-8B-V1',\n",
    "    'Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv': 'Llama-3.2-3B-V1',\n",
    "    'phi-4-unsloth-bnb-4bit-V1.csv': 'Phi-4-14B-V1',\n",
    "    'zephyr-sft-bnb-4bit-V1.csv': 'Zephyr-7B-V1',\n",
    "    'Qwen2.5-7B-Instruct-bnb-4bit-V1.csv': 'Qwen2.5-7B-V1',\n",
    "    'Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv': 'Qwen2.5-14B-V1',\n",
    "}\n",
    "\n",
    "df_answer = pd.read_csv(answer_path)[['ID', 'ANSWER']]\n",
    "answer_mapping = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}\n",
    "df_answer['ANSWER'] = df_answer['ANSWER'].map(answer_mapping)\n",
    "subject = pd.read_csv(\"/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu.csv\")\n",
    "df_answer = df_answer.merge(subject[['question_id', 'subject']], left_on='ID', right_on='question_id', how='left')\n",
    "\n",
    "\n",
    "def extrair_model_answer(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return 'invalid'\n",
    "    match = re.search(r'[ABCD]', texto)\n",
    "    return match.group(0) if match else 'invalid'\n",
    "\n",
    "def processar_dataframe(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['output_model'] = df['output_model'].str.upper()\n",
    "    \n",
    "    df['last_answer'] = df['output_model'].str.extract(\n",
    "        r'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\s*(.*)', flags=re.IGNORECASE\n",
    "    ).fillna('invalid')\n",
    "    \n",
    "    df['model_answer'] = df['last_answer'].apply(extrair_model_answer)\n",
    "    df['len_prompt'] = df['prompt'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df['len_output'] = df['output_model'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    \n",
    "    df = df.merge(df_answer, on='ID', how='left')\n",
    "    \n",
    "    df['is_error'] = (df['model_answer'] != df['ANSWER']).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "resultados_estatisticas = []\n",
    "\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith('.csv') and filename in models_map:\n",
    "        model_name = models_map[filename]\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        print(f\"Processando: {model_name}\")\n",
    "        \n",
    "        df = processar_dataframe(file_path)\n",
    "        \n",
    "        stats = df.groupby('is_error')[['len_prompt', 'len_output']].agg(\n",
    "            len_prompt_mean=('len_prompt', 'mean'),\n",
    "            len_prompt_median=('len_prompt', 'median'),\n",
    "            len_prompt_std=('len_prompt', 'std'),\n",
    "            len_output_mean=('len_output', 'mean'),\n",
    "            len_output_median=('len_output', 'median'),\n",
    "            len_output_std=('len_output', 'std'),\n",
    "            total=('len_prompt', 'count')\n",
    "        ).reset_index()\n",
    "        \n",
    "        stats['modelo'] = model_name\n",
    "        resultados_estatisticas.append(stats)\n",
    "\n",
    "\n",
    "df_estatisticas = pd.concat(resultados_estatisticas, ignore_index=True)\n",
    "cols = ['modelo', 'is_error'] + [col for col in df_estatisticas.columns if col not in ['modelo', 'is_error']]\n",
    "df_estatisticas = df_estatisticas[cols]\n",
    "df_estatisticas.to_csv('estatisticas_erros_modelos_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9d9c8",
   "metadata": {},
   "source": [
    "## Violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53233e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_5653/2042539408.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  '''import os\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'import os\\nimport pandas as pd\\nimport plotly.express as px\\nfrom plotly.subplots import make_subplots\\nimport re\\n\\ndef salvar_html(fig, caminho):\\n    try:\\n        os.makedirs(os.path.dirname(caminho), exist_ok=True)\\n        fig.write_html(caminho)\\n        print(f\"Salvo HTML em: {caminho}\")\\n    except Exception as e:\\n        print(f\"Erro ao salvar HTML: {e}\")\\n\\ndef extrair_model_answer(texto):\\n    if not isinstance(texto, str):\\n        return \\'invalid\\'\\n    match = re.search(r\\'[ABCD]\\', texto)\\n    return match.group(0) if match else \\'invalid\\'\\n\\ndef processar_dataframe(filepath):\\n    df = pd.read_csv(filepath)\\n    df[\\'output_model\\'] = df[\\'output_model\\'].str.upper()\\n    df[\\'last_answer\\'] = df[\\'output_model\\'].str.extract(\\n        r\\'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\\\s*(.*)\\', flags=re.IGNORECASE\\n    ).fillna(\\'invalid\\')\\n    df[\\'model_answer\\'] = df[\\'last_answer\\'].apply(extrair_model_answer)\\n    df[\\'len_prompt\\'] = df[\\'prompt\\'].apply(lambda x: len(x) if isinstance(x, str) else 0)\\n    df[\\'len_output\\'] = df[\\'output_model\\'].apply(lambda x: len(x) if isinstance(x, str) else 0)\\n    df = df.merge(df_answer, on=\\'ID\\', how=\\'left\\')\\n    df[\\'is_error\\'] = (df[\\'model_answer\\'] != df[\\'ANSWER\\']).astype(int)\\n    df[\\'resultado\\'] = df[\\'is_error\\'].map({0: \\'Acerto\\', 1: \\'Erro\\'})\\n    return df\\n\\ndef criar_violin_plot(df, coluna, row, fig):\\n    color_map = {\\n        \"Acerto\": \"#1f77b4\",  # Azul\\n        \"Erro\": \"#d62728\"     # Vermelho\\n    }\\n    violin = px.violin(\\n        df,\\n        y=coluna,\\n        x=\"resultado\",\\n        color=\"resultado\",\\n        box=True,\\n        points=\"all\",\\n        color_discrete_map=color_map\\n    )\\n    for trace in violin.data:\\n        fig.add_trace(trace, row=row, col=1)\\n    fig.update_yaxes(title_text=\"Tamanho\", row=row, col=1)\\n\\n\\ndef plot_violin_duplo(df, model_name):\\n    fig = make_subplots(\\n        rows=2,\\n        cols=1,\\n        shared_xaxes=True,\\n        vertical_spacing=0.1,\\n        subplot_titles=[\\n            \"Distribuição do Tamanho do Prompt de Entrada\",\\n            \"Distribuição do Tamanho do Prompt de Saída do Modelo\"\\n        ]\\n    )\\n\\n    criar_violin_plot(df, \"len_prompt\", row=1, fig=fig)\\n    criar_violin_plot(df, \"len_output\", row=2, fig=fig)\\n\\n    fig.update_layout(\\n        height=800,\\n        width=1000,\\n        title_text=f\"Distribuições - {model_name}\",\\n        showlegend=False,\\n        font=dict(color=\"black\", size=16),\\n        plot_bgcolor=\"#f9f9f9\",\\n        paper_bgcolor=\"white\"\\n    )\\n    fig.update_xaxes(title_text=\"Resultado (Acerto ou Erro)\", row=2, col=1)\\n\\n    salvar_html(fig, f\"/home/annap/Downloads/01_plots_html/dist_{model_name}_violin.html\")\\n\\n# Caminhos\\ndir_path = \\'../UNSLOTH/V1/GLOSA_PT\\'\\nanswer_path = \\'/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv\\'\\n\\n# Mapeamento dos nomes\\nmodels_map = {\\n    \\'Llama-3.1-8B-unsloth-bnb-4bit-V1.csv\\': \\'Llama-3.1-8B-V1\\',\\n    \\'Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv\\': \\'Llama-3.2-3B-V1\\',\\n    \\'phi-4-unsloth-bnb-4bit-V1.csv\\': \\'Phi-4-14B-V1\\',\\n    \\'zephyr-sft-bnb-4bit-V1.csv\\': \\'Zephyr-7B-V1\\',\\n    \\'Qwen2.5-7B-Instruct-bnb-4bit-V1.csv\\': \\'Qwen2.5-7B-V1\\',\\n    \\'Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv\\': \\'Qwen2.5-14B-V1\\',\\n}\\n\\n# Carrega respostas\\ndf_answer = pd.read_csv(answer_path)[[\\'ID\\', \\'ANSWER\\']]\\ndf_answer[\\'ANSWER\\'] = df_answer[\\'ANSWER\\'].map({0: \\'A\\', 1: \\'B\\', 2: \\'C\\', 3: \\'D\\'})\\n\\n# Loop para processar cada modelo\\nfor filename in os.listdir(dir_path):\\n    if filename.endswith(\\'.csv\\') and filename in models_map:\\n        model_name = models_map[filename]\\n        print(f\"Processando: {model_name}\")\\n        file_path = os.path.join(dir_path, filename)\\n        df = processar_dataframe(file_path)\\n        plot_violin_duplo(df, model_name)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "\n",
    "def salvar_html(fig, caminho):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(caminho), exist_ok=True)\n",
    "        fig.write_html(caminho)\n",
    "        print(f\"Salvo HTML em: {caminho}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar HTML: {e}\")\n",
    "\n",
    "def extrair_model_answer(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return 'invalid'\n",
    "    match = re.search(r'[ABCD]', texto)\n",
    "    return match.group(0) if match else 'invalid'\n",
    "\n",
    "def processar_dataframe(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['output_model'] = df['output_model'].str.upper()\n",
    "    df['last_answer'] = df['output_model'].str.extract(\n",
    "        r'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\s*(.*)', flags=re.IGNORECASE\n",
    "    ).fillna('invalid')\n",
    "    df['model_answer'] = df['last_answer'].apply(extrair_model_answer)\n",
    "    df['len_prompt'] = df['prompt'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df['len_output'] = df['output_model'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df = df.merge(df_answer, on='ID', how='left')\n",
    "    df['is_error'] = (df['model_answer'] != df['ANSWER']).astype(int)\n",
    "    df['resultado'] = df['is_error'].map({0: 'Acerto', 1: 'Erro'})\n",
    "    return df\n",
    "\n",
    "def criar_violin_plot(df, coluna, row, fig):\n",
    "    color_map = {\n",
    "        \"Acerto\": \"#1f77b4\",  # Azul\n",
    "        \"Erro\": \"#d62728\"     # Vermelho\n",
    "    }\n",
    "    violin = px.violin(\n",
    "        df,\n",
    "        y=coluna,\n",
    "        x=\"resultado\",\n",
    "        color=\"resultado\",\n",
    "        box=True,\n",
    "        points=\"all\",\n",
    "        color_discrete_map=color_map\n",
    "    )\n",
    "    for trace in violin.data:\n",
    "        fig.add_trace(trace, row=row, col=1)\n",
    "    fig.update_yaxes(title_text=\"Tamanho\", row=row, col=1)\n",
    "\n",
    "\n",
    "def plot_violin_duplo(df, model_name):\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.1,\n",
    "        subplot_titles=[\n",
    "            \"Distribuição do Tamanho do Prompt de Entrada\",\n",
    "            \"Distribuição do Tamanho do Prompt de Saída do Modelo\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    criar_violin_plot(df, \"len_prompt\", row=1, fig=fig)\n",
    "    criar_violin_plot(df, \"len_output\", row=2, fig=fig)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        title_text=f\"Distribuições - {model_name}\",\n",
    "        showlegend=False,\n",
    "        font=dict(color=\"black\", size=16),\n",
    "        plot_bgcolor=\"#f9f9f9\",\n",
    "        paper_bgcolor=\"white\"\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Resultado (Acerto ou Erro)\", row=2, col=1)\n",
    "\n",
    "    salvar_html(fig, f\"/home/annap/Downloads/01_plots_html/dist_{model_name}_violin.html\")\n",
    "\n",
    "# Caminhos\n",
    "dir_path = '../UNSLOTH/V1/GLOSA_PT'\n",
    "answer_path = '/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv'\n",
    "\n",
    "# Mapeamento dos nomes\n",
    "models_map = {\n",
    "    'Llama-3.1-8B-unsloth-bnb-4bit-V1.csv': 'Llama-3.1-8B-V1',\n",
    "    'Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv': 'Llama-3.2-3B-V1',\n",
    "    'phi-4-unsloth-bnb-4bit-V1.csv': 'Phi-4-14B-V1',\n",
    "    'zephyr-sft-bnb-4bit-V1.csv': 'Zephyr-7B-V1',\n",
    "    'Qwen2.5-7B-Instruct-bnb-4bit-V1.csv': 'Qwen2.5-7B-V1',\n",
    "    'Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv': 'Qwen2.5-14B-V1',\n",
    "}\n",
    "\n",
    "# Carrega respostas\n",
    "df_answer = pd.read_csv(answer_path)[['ID', 'ANSWER']]\n",
    "df_answer['ANSWER'] = df_answer['ANSWER'].map({0: 'A', 1: 'B', 2: 'C', 3: 'D'})\n",
    "\n",
    "# Loop para processar cada modelo\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith('.csv') and filename in models_map:\n",
    "        model_name = models_map[filename]\n",
    "        print(f\"Processando: {model_name}\")\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        df = processar_dataframe(file_path)\n",
    "        plot_violin_duplo(df, model_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1994d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8378b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo: /home/annap/Downloads/01_plots_html/compare_Llama-3.1-8B.html\n",
      "Salvo: /home/annap/Downloads/01_plots_html/compare_Llama-3.2-3B.html\n",
      "Salvo: /home/annap/Downloads/01_plots_html/compare_Qwen2.5-7B.html\n",
      "Salvo: /home/annap/Downloads/01_plots_html/compare_Qwen2.5-14B.html\n",
      "Salvo: /home/annap/Downloads/01_plots_html/compare_Phi-4-14B.html\n",
      "Salvo: /home/annap/Downloads/01_plots_html/compare_Zephyr-7B.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extrair_model_answer(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return 'invalid'\n",
    "    match = re.search(r'[ABCD]', texto)\n",
    "    return match.group(0) if match else 'invalid'\n",
    "\n",
    "def processar_dataframe(filepath, versao, modelo_base):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['output_model'] = df['output_model'].str.upper()\n",
    "    df['last_answer'] = df['output_model'].str.extract(r'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\s*(.*)', flags=re.IGNORECASE).fillna('invalid')\n",
    "    df['model_answer'] = df['last_answer'].apply(extrair_model_answer)\n",
    "    df['len_prompt'] = df['prompt'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df['len_output'] = df['output_model'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df = df.merge(df_answer, on='ID', how='left')\n",
    "    df['is_error'] = (df['model_answer'] != df['ANSWER']).astype(int)\n",
    "    df['resultado'] = df['is_error'].map({0: 'Acerto', 1: 'Erro'})\n",
    "    df['versao'] = versao\n",
    "    df['modelo_base'] = modelo_base\n",
    "    return df\n",
    "\n",
    "def plot_comparativo_violin(df_model, modelo_base):\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Tamanho do Prompt\", \"Tamanho da Saída\"],\n",
    "        shared_yaxes=False,\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "\n",
    "    for col, col_idx in zip([\"len_prompt\", \"len_output\"], [1, 2]):\n",
    "        fig_tmp = px.violin(\n",
    "            df_model,\n",
    "            y=col,\n",
    "            x=\"versao\",\n",
    "            color=\"resultado\",\n",
    "            box=True,\n",
    "            points=\"all\",\n",
    "            category_orders={\"versao\": [\"V0\", \"V1\"]}\n",
    "        )\n",
    "        for trace in fig_tmp.data:\n",
    "            fig.add_trace(trace, row=1, col=col_idx)\n",
    "\n",
    "        fig.update_yaxes(title_text=\"Tamanho\", row=1, col=col_idx)\n",
    "        fig.update_xaxes(title_text=\"Versão\", row=1, col=col_idx)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Distribuição de Prompt e Output - {modelo_base}\",\n",
    "        font=dict(size=14),\n",
    "        height=500,\n",
    "        width=1000,\n",
    "        plot_bgcolor=\"#f9f9f9\",\n",
    "        paper_bgcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    output_path = f\"/home/annap/Downloads/01_plots_html/compare_{modelo_base}.html\"\n",
    "    fig.write_html(output_path)\n",
    "    print(f\"Salvo: {output_path}\")\n",
    "\n",
    "# Caminhos\n",
    "dir_path = \"../UNSLOTH/V1/GLOSA_PT\"\n",
    "answer_path = \"/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv\"\n",
    "\n",
    "# Respostas\n",
    "df_answer = pd.read_csv(answer_path)[['ID', 'ANSWER']]\n",
    "df_answer['ANSWER'] = df_answer['ANSWER'].map({0: 'A', 1: 'B', 2: 'C', 3: 'D'})\n",
    "\n",
    "# Mapeamento por modelo base\n",
    "modelos_base = {\n",
    "    \"Llama-3.1-8B\": [\"Llama-3.1-8B-unsloth-bnb-4bit.csv\", \"Llama-3.1-8B-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Llama-3.2-3B\": [\"Llama-3.2-3B-Instruct-unsloth-bnb-4bit.csv\", \"Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-7B\": [\"Qwen2.5-7B-Instruct-bnb-4bit.csv\", \"Qwen2.5-7B-Instruct-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-14B\": [\"Qwen2.5-14B-Instruct-unsloth-bnb-4bit.csv\", \"Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Phi-4-14B\": [\"phi-4-unsloth-bnb-4bit.csv\", \"phi-4-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Zephyr-7B\": [\"zephyr-sft-bnb-4bit.csv\", \"zephyr-sft-bnb-4bit-V1.csv\"],\n",
    "}\n",
    "\n",
    "# Gera os plots\n",
    "for modelo_base, arquivos in modelos_base.items():\n",
    "    dfs = []\n",
    "    for i, arquivo in enumerate(arquivos):\n",
    "        versao = f\"V{i}\"\n",
    "        path = os.path.join(dir_path, arquivo)\n",
    "        if os.path.exists(path):\n",
    "            df = processar_dataframe(path, versao, modelo_base)\n",
    "            dfs.append(df)\n",
    "    if dfs:\n",
    "        df_modelo = pd.concat(dfs, ignore_index=True)\n",
    "        plot_comparativo_violin(df_modelo, modelo_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "531cb07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n",
      "<>:1: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n",
      "/tmp/ipykernel_5653/116298664.py:1: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'import os\\nimport pandas as pd\\nimport plotly.express as px\\nfrom plotly.subplots import make_subplots\\nimport plotly.graph_objects as go\\nimport re\\n\\ndef extrair_model_answer(texto):\\n    if not isinstance(texto, str):\\n        return \\'invalid\\'\\n    match = re.search(r\\'[ABCD]\\', texto)\\n    return match.group(0) if match else \\'invalid\\'\\n\\ndef processar_dataframe(filepath, versao, modelo_base, df_answer):\\n    df = pd.read_csv(filepath)\\n    df[\\'output_model\\'] = df[\\'output_model\\'].str.upper()\\n    df[\\'last_answer\\'] = df[\\'output_model\\'].str.extract(r\\'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\\\s*(.*)\\', flags=re.IGNORECASE).fillna(\\'invalid\\')\\n    df[\\'model_answer\\'] = df[\\'last_answer\\'].apply(extrair_model_answer)\\n    df[\\'len_prompt\\'] = df[\\'prompt\\'].apply(lambda x: len(x) if isinstance(x, str) else 0)\\n    df[\\'len_output\\'] = df[\\'output_model\\'].apply(lambda x: len(x) if isinstance(x, str) else 0)\\n    df = df.merge(df_answer, on=\\'ID\\', how=\\'left\\')\\n    df[\\'is_error\\'] = (df[\\'model_answer\\'] != df[\\'ANSWER\\']).astype(int)\\n    df[\\'versao\\'] = versao\\n    df[\\'modelo_base\\'] = modelo_base\\n    return df[df[\\'is_error\\'] == 1]  # <-- apenas erros!\\n\\ndef plot_violin_erros(df_model, modelo_base):\\n    fig = make_subplots(\\n        rows=1, cols=2,\\n        subplot_titles=[\"Tamanho do Prompt (Erro)\", \"Tamanho do Output (Erro)\"],\\n        shared_yaxes=False,\\n        horizontal_spacing=0.15\\n    )\\n\\n    for col, idx in zip([\"len_prompt\", \"len_output\"], [1, 2]):\\n        fig_tmp = px.violin(\\n            df_model,\\n            y=col,\\n            x=\"versao\",\\n            color=\"versao\",\\n            box=True,\\n            points=\"all\",\\n            category_orders={\"versao\": [\"V0\", \"V1\"]},\\n            color_discrete_map={\"V0\": \"#636EFA\", \"V1\": \"#EF553B\"}\\n        )\\n        for trace in fig_tmp.data:\\n            fig.add_trace(trace, row=1, col=idx)\\n        fig.update_yaxes(title_text=\"Tamanho\", row=1, col=idx)\\n        fig.update_xaxes(title_text=\"Versão\", row=1, col=idx)\\n\\n    fig.update_layout(\\n        title_text=f\"Distribuições de Tamanho (Erros) - {modelo_base}\",\\n        font=dict(size=14),\\n        height=500,\\n        width=1000,\\n        plot_bgcolor=\"#f9f9f9\",\\n        paper_bgcolor=\"white\",\\n        showlegend=False\\n    )\\n\\n    output_path = f\"/home/annap/Downloads/01_plots_html/erros_{modelo_base}.html\"\\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\\n    fig.write_html(output_path)\\n    print(f\"Salvo: {output_path}\")\\n\\n# Caminhos e arquivos\\nv0_dir = \\'../UNSLOTH/NO_FINE_TUNE/GLOSA_PT\\'\\nv1_dir = \\'../UNSLOTH/V1/GLOSA_PT\\'\\nanswer_path = \\'/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv\\'\\n\\n# Mapeamento dos arquivos para cada modelo base\\nmodelos_base = {\\n    \"Llama-3.1-8B\": [\"Llama-3.1-8B-unsloth-bnb-4bit.csv\", \"Llama-3.1-8B-unsloth-bnb-4bit-V1.csv\"],\\n    \"Llama-3.2-3B\": [\"Llama-3.2-3B-Instruct-unsloth-bnb-4bit.csv\", \"Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv\"],\\n    \"Qwen2.5-7B\": [\"Qwen2.5-7B-Instruct-bnb-4bit.csv\", \"Qwen2.5-7B-Instruct-bnb-4bit-V1.csv\"],\\n    \"Qwen2.5-14B\": [\"Qwen2.5-14B-Instruct-unsloth-bnb-4bit.csv\", \"Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv\"],\\n    \"Phi-4-14B\": [\"phi-4-unsloth-bnb-4bit.csv\", \"phi-4-unsloth-bnb-4bit-V1.csv\"],\\n    \"Zephyr-7B\": [\"zephyr-sft-bnb-4bit.csv\", \"zephyr-sft-bnb-4bit-V1.csv\"],\\n}\\n\\n# Carrega respostas\\ndf_answer = pd.read_csv(answer_path)[[\\'ID\\', \\'ANSWER\\']]\\ndf_answer[\\'ANSWER\\'] = df_answer[\\'ANSWER\\'].map({0: \\'A\\', 1: \\'B\\', 2: \\'C\\', 3: \\'D\\'})\\n\\n# Processa e plota por modelo\\nfor modelo_base, (v0_file, v1_file) in modelos_base.items():\\n    dfs = []\\n    v0_path = os.path.join(v0_dir, v0_file)\\n    v1_path = os.path.join(v1_dir, v1_file)\\n\\n    if os.path.exists(v0_path):\\n        dfs.append(processar_dataframe(v0_path, \\'V0\\', modelo_base, df_answer))\\n    if os.path.exists(v1_path):\\n        dfs.append(processar_dataframe(v1_path, \\'V1\\', modelo_base, df_answer))\\n\\n    if dfs:\\n        df_model = pd.concat(dfs, ignore_index=True)\\n        plot_violin_erros(df_model, modelo_base)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "\n",
    "def extrair_model_answer(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return 'invalid'\n",
    "    match = re.search(r'[ABCD]', texto)\n",
    "    return match.group(0) if match else 'invalid'\n",
    "\n",
    "def processar_dataframe(filepath, versao, modelo_base, df_answer):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['output_model'] = df['output_model'].str.upper()\n",
    "    df['last_answer'] = df['output_model'].str.extract(r'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\s*(.*)', flags=re.IGNORECASE).fillna('invalid')\n",
    "    df['model_answer'] = df['last_answer'].apply(extrair_model_answer)\n",
    "    df['len_prompt'] = df['prompt'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df['len_output'] = df['output_model'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df = df.merge(df_answer, on='ID', how='left')\n",
    "    df['is_error'] = (df['model_answer'] != df['ANSWER']).astype(int)\n",
    "    df['versao'] = versao\n",
    "    df['modelo_base'] = modelo_base\n",
    "    return df[df['is_error'] == 1]  # <-- apenas erros!\n",
    "\n",
    "def plot_violin_erros(df_model, modelo_base):\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Tamanho do Prompt (Erro)\", \"Tamanho do Output (Erro)\"],\n",
    "        shared_yaxes=False,\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "\n",
    "    for col, idx in zip([\"len_prompt\", \"len_output\"], [1, 2]):\n",
    "        fig_tmp = px.violin(\n",
    "            df_model,\n",
    "            y=col,\n",
    "            x=\"versao\",\n",
    "            color=\"versao\",\n",
    "            box=True,\n",
    "            points=\"all\",\n",
    "            category_orders={\"versao\": [\"V0\", \"V1\"]},\n",
    "            color_discrete_map={\"V0\": \"#636EFA\", \"V1\": \"#EF553B\"}\n",
    "        )\n",
    "        for trace in fig_tmp.data:\n",
    "            fig.add_trace(trace, row=1, col=idx)\n",
    "        fig.update_yaxes(title_text=\"Tamanho\", row=1, col=idx)\n",
    "        fig.update_xaxes(title_text=\"Versão\", row=1, col=idx)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Distribuições de Tamanho (Erros) - {modelo_base}\",\n",
    "        font=dict(size=14),\n",
    "        height=500,\n",
    "        width=1000,\n",
    "        plot_bgcolor=\"#f9f9f9\",\n",
    "        paper_bgcolor=\"white\",\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    output_path = f\"/home/annap/Downloads/01_plots_html/erros_{modelo_base}.html\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    fig.write_html(output_path)\n",
    "    print(f\"Salvo: {output_path}\")\n",
    "\n",
    "# Caminhos e arquivos\n",
    "v0_dir = '../UNSLOTH/NO_FINE_TUNE/GLOSA_PT'\n",
    "v1_dir = '../UNSLOTH/V1/GLOSA_PT'\n",
    "answer_path = '/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv'\n",
    "\n",
    "# Mapeamento dos arquivos para cada modelo base\n",
    "modelos_base = {\n",
    "    \"Llama-3.1-8B\": [\"Llama-3.1-8B-unsloth-bnb-4bit.csv\", \"Llama-3.1-8B-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Llama-3.2-3B\": [\"Llama-3.2-3B-Instruct-unsloth-bnb-4bit.csv\", \"Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-7B\": [\"Qwen2.5-7B-Instruct-bnb-4bit.csv\", \"Qwen2.5-7B-Instruct-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-14B\": [\"Qwen2.5-14B-Instruct-unsloth-bnb-4bit.csv\", \"Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Phi-4-14B\": [\"phi-4-unsloth-bnb-4bit.csv\", \"phi-4-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Zephyr-7B\": [\"zephyr-sft-bnb-4bit.csv\", \"zephyr-sft-bnb-4bit-V1.csv\"],\n",
    "}\n",
    "\n",
    "# Carrega respostas\n",
    "df_answer = pd.read_csv(answer_path)[['ID', 'ANSWER']]\n",
    "df_answer['ANSWER'] = df_answer['ANSWER'].map({0: 'A', 1: 'B', 2: 'C', 3: 'D'})\n",
    "\n",
    "# Processa e plota por modelo\n",
    "for modelo_base, (v0_file, v1_file) in modelos_base.items():\n",
    "    dfs = []\n",
    "    v0_path = os.path.join(v0_dir, v0_file)\n",
    "    v1_path = os.path.join(v1_dir, v1_file)\n",
    "\n",
    "    if os.path.exists(v0_path):\n",
    "        dfs.append(processar_dataframe(v0_path, 'V0', modelo_base, df_answer))\n",
    "    if os.path.exists(v1_path):\n",
    "        dfs.append(processar_dataframe(v1_path, 'V1', modelo_base, df_answer))\n",
    "\n",
    "    if dfs:\n",
    "        df_model = pd.concat(dfs, ignore_index=True)\n",
    "        plot_violin_erros(df_model, modelo_base)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708b01a",
   "metadata": {},
   "source": [
    "## V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e32fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n",
      "<>:1: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n",
      "/tmp/ipykernel_5653/1323320740.py:1: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'import os\\nimport pandas as pd\\nimport plotly.express as px\\nfrom plotly.subplots import make_subplots\\nimport plotly.graph_objects as go\\nimport re\\n\\ndef extrair_model_answer(texto):\\n    if not isinstance(texto, str):\\n        return \\'invalid\\'\\n    match = re.search(r\\'[ABCD]\\', texto)\\n    return match.group(0) if match else \\'invalid\\'\\n\\ndef processar_dataframe(filepath, versao, modelo_base, df_answer):\\n    df = pd.read_csv(filepath)\\n    df[\\'output_model\\'] = df[\\'output_model\\'].str.upper()\\n    df[\\'last_answer\\'] = df[\\'output_model\\'].str.extract(r\\'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\\\s*(.*)\\', flags=re.IGNORECASE).fillna(\\'invalid\\')\\n    df[\\'model_answer\\'] = df[\\'last_answer\\'].apply(extrair_model_answer)\\n    df[\\'len_prompt\\'] = df[\\'prompt\\'].apply(lambda x: len(x) if isinstance(x, str) else 0)\\n    df[\\'len_output\\'] = df[\\'output_model\\'].apply(lambda x: len(x) if isinstance(x, str) else 0)\\n    df = df.merge(df_answer, on=\\'ID\\', how=\\'left\\')\\n    df[\\'is_error\\'] = (df[\\'model_answer\\'] != df[\\'ANSWER\\']).astype(int)\\n    df[\\'versao\\'] = versao\\n    df[\\'modelo_base\\'] = modelo_base\\n    return df[df[\\'is_error\\'] == 1]  # Apenas erros\\n\\ndef plot_violin_erros(df_model, modelo_base):\\n    fig = make_subplots(\\n        rows=1, cols=2,\\n        subplot_titles=[\"Tamanho do Prompt dos Erros\", \"Tamanho do Output dos Erros\"],\\n        horizontal_spacing=0.15\\n    )\\n\\n    color_map = {\"V0\": \"#636EFA\", \"V1\": \"#EF553B\"}\\n\\n    for col, idx in zip([\"len_prompt\", \"len_output\"], [1, 2]):\\n        violin_fig = px.violin(\\n            df_model,\\n            y=col,\\n            color=\"versao\",\\n            box=True,\\n            points=\"all\",\\n            color_discrete_map=color_map,\\n            category_orders={\"versao\": [\"V0\", \"V1\"]}\\n        )\\n        for trace in violin_fig.data:\\n            fig.add_trace(trace, row=1, col=idx)\\n\\n        fig.update_yaxes(title_text=\"Tamanho\", row=1, col=idx)\\n        fig.update_xaxes(title_text=\"\", row=1, col=idx, showticklabels=False)\\n\\n    fig.update_layout(\\n        title_text=f\"Distribuição de Tamanhos (Erros) - {modelo_base}\",\\n        font=dict(size=14),\\n        height=500,\\n        width=1000,\\n        plot_bgcolor=\"#f9f9f9\",\\n        paper_bgcolor=\"white\",\\n        showlegend=True,\\n        legend=dict(title=\"Versão\", orientation=\"h\", y=-0.2, x=0.25)\\n    )\\n\\n    output_path = f\"/home/annap/Downloads/01_plots_html/erros_{modelo_base}.html\"\\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\\n    fig.write_html(output_path)\\n    print(f\"Salvo: {output_path}\")\\n\\n# Caminhos\\nv0_dir = \\'../UNSLOTH/NO_FINE_TUNE/GLOSA_PT\\'\\nv1_dir = \\'../UNSLOTH/V1/GLOSA_PT\\'\\nanswer_path = \\'/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv\\'\\n\\n# Mapeamento dos arquivos para cada modelo base\\nmodelos_base = {\\n    \"Llama-3.1-8B\": [\"Llama-3.1-8B-unsloth-bnb-4bit.csv\", \"Llama-3.1-8B-unsloth-bnb-4bit-V1.csv\"],\\n    \"Llama-3.2-3B\": [\"Llama-3.2-3B-Instruct-unsloth-bnb-4bit.csv\", \"Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv\"],\\n    \"Qwen2.5-7B\": [\"Qwen2.5-7B-Instruct-bnb-4bit.csv\", \"Qwen2.5-7B-Instruct-bnb-4bit-V1.csv\"],\\n    \"Qwen2.5-14B\": [\"Qwen2.5-14B-Instruct-unsloth-bnb-4bit.csv\", \"Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv\"],\\n    \"Phi-4-14B\": [\"phi-4-unsloth-bnb-4bit.csv\", \"phi-4-unsloth-bnb-4bit-V1.csv\"],\\n    \"Zephyr-7B\": [\"zephyr-sft-bnb-4bit.csv\", \"zephyr-sft-bnb-4bit-V1.csv\"],\\n}\\n\\n# Respostas corretas\\ndf_answer = pd.read_csv(answer_path)[[\\'ID\\', \\'ANSWER\\']]\\ndf_answer[\\'ANSWER\\'] = df_answer[\\'ANSWER\\'].map({0: \\'A\\', 1: \\'B\\', 2: \\'C\\', 3: \\'D\\'})\\n\\n# Loop principal\\nfor modelo_base, (v0_file, v1_file) in modelos_base.items():\\n    dfs = []\\n    v0_path = os.path.join(v0_dir, v0_file)\\n    v1_path = os.path.join(v1_dir, v1_file)\\n\\n    if os.path.exists(v0_path):\\n        dfs.append(processar_dataframe(v0_path, \\'V0\\', modelo_base, df_answer))\\n    if os.path.exists(v1_path):\\n        dfs.append(processar_dataframe(v1_path, \\'V1\\', modelo_base, df_answer))\\n\\n    if dfs:\\n        df_model = pd.concat(dfs, ignore_index=True)\\n        plot_violin_erros(df_model, modelo_base)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "\n",
    "def extrair_model_answer(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return 'invalid'\n",
    "    match = re.search(r'[ABCD]', texto)\n",
    "    return match.group(0) if match else 'invalid'\n",
    "\n",
    "def processar_dataframe(filepath, versao, modelo_base, df_answer):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['output_model'] = df['output_model'].str.upper()\n",
    "    df['last_answer'] = df['output_model'].str.extract(r'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\s*(.*)', flags=re.IGNORECASE).fillna('invalid')\n",
    "    df['model_answer'] = df['last_answer'].apply(extrair_model_answer)\n",
    "    df['len_prompt'] = df['prompt'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df['len_output'] = df['output_model'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df = df.merge(df_answer, on='ID', how='left')\n",
    "    df['is_error'] = (df['model_answer'] != df['ANSWER']).astype(int)\n",
    "    df['versao'] = versao\n",
    "    df['modelo_base'] = modelo_base\n",
    "    return df[df['is_error'] == 1]  # Apenas erros\n",
    "\n",
    "def plot_violin_erros(df_model, modelo_base):\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Tamanho do Prompt dos Erros\", \"Tamanho do Output dos Erros\"],\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "\n",
    "    color_map = {\"V0\": \"#636EFA\", \"V1\": \"#EF553B\"}\n",
    "\n",
    "    for col, idx in zip([\"len_prompt\", \"len_output\"], [1, 2]):\n",
    "        violin_fig = px.violin(\n",
    "            df_model,\n",
    "            y=col,\n",
    "            color=\"versao\",\n",
    "            box=True,\n",
    "            points=\"all\",\n",
    "            color_discrete_map=color_map,\n",
    "            category_orders={\"versao\": [\"V0\", \"V1\"]}\n",
    "        )\n",
    "        for trace in violin_fig.data:\n",
    "            fig.add_trace(trace, row=1, col=idx)\n",
    "\n",
    "        fig.update_yaxes(title_text=\"Tamanho\", row=1, col=idx)\n",
    "        fig.update_xaxes(title_text=\"\", row=1, col=idx, showticklabels=False)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Distribuição de Tamanhos (Erros) - {modelo_base}\",\n",
    "        font=dict(size=14),\n",
    "        height=500,\n",
    "        width=1000,\n",
    "        plot_bgcolor=\"#f9f9f9\",\n",
    "        paper_bgcolor=\"white\",\n",
    "        showlegend=True,\n",
    "        legend=dict(title=\"Versão\", orientation=\"h\", y=-0.2, x=0.25)\n",
    "    )\n",
    "\n",
    "    output_path = f\"/home/annap/Downloads/01_plots_html/erros_{modelo_base}.html\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    fig.write_html(output_path)\n",
    "    print(f\"Salvo: {output_path}\")\n",
    "\n",
    "# Caminhos\n",
    "v0_dir = '../UNSLOTH/NO_FINE_TUNE/GLOSA_PT'\n",
    "v1_dir = '../UNSLOTH/V1/GLOSA_PT'\n",
    "answer_path = '/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv'\n",
    "\n",
    "# Mapeamento dos arquivos para cada modelo base\n",
    "modelos_base = {\n",
    "    \"Llama-3.1-8B\": [\"Llama-3.1-8B-unsloth-bnb-4bit.csv\", \"Llama-3.1-8B-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Llama-3.2-3B\": [\"Llama-3.2-3B-Instruct-unsloth-bnb-4bit.csv\", \"Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-7B\": [\"Qwen2.5-7B-Instruct-bnb-4bit.csv\", \"Qwen2.5-7B-Instruct-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-14B\": [\"Qwen2.5-14B-Instruct-unsloth-bnb-4bit.csv\", \"Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Phi-4-14B\": [\"phi-4-unsloth-bnb-4bit.csv\", \"phi-4-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Zephyr-7B\": [\"zephyr-sft-bnb-4bit.csv\", \"zephyr-sft-bnb-4bit-V1.csv\"],\n",
    "}\n",
    "\n",
    "# Respostas corretas\n",
    "df_answer = pd.read_csv(answer_path)[['ID', 'ANSWER']]\n",
    "df_answer['ANSWER'] = df_answer['ANSWER'].map({0: 'A', 1: 'B', 2: 'C', 3: 'D'})\n",
    "\n",
    "# Loop principal\n",
    "for modelo_base, (v0_file, v1_file) in modelos_base.items():\n",
    "    dfs = []\n",
    "    v0_path = os.path.join(v0_dir, v0_file)\n",
    "    v1_path = os.path.join(v1_dir, v1_file)\n",
    "\n",
    "    if os.path.exists(v0_path):\n",
    "        dfs.append(processar_dataframe(v0_path, 'V0', modelo_base, df_answer))\n",
    "    if os.path.exists(v1_path):\n",
    "        dfs.append(processar_dataframe(v1_path, 'V1', modelo_base, df_answer))\n",
    "\n",
    "    if dfs:\n",
    "        df_model = pd.concat(dfs, ignore_index=True)\n",
    "        plot_violin_erros(df_model, modelo_base)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb3e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando: Llama-3.2-3B-V1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "processar_dataframe() missing 2 required positional arguments: 'versao' and 'modelo_base'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m file_path = os.path.join(dir_path, filename)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessando: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df = \u001b[43mprocessar_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m stats = df.groupby(\u001b[33m'\u001b[39m\u001b[33mis_error\u001b[39m\u001b[33m'\u001b[39m)[[\u001b[33m'\u001b[39m\u001b[33mlen_prompt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlen_output\u001b[39m\u001b[33m'\u001b[39m]].agg(\n\u001b[32m     12\u001b[39m     len_prompt_mean=(\u001b[33m'\u001b[39m\u001b[33mlen_prompt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     13\u001b[39m     len_prompt_median=(\u001b[33m'\u001b[39m\u001b[33mlen_prompt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     total=(\u001b[33m'\u001b[39m\u001b[33mlen_prompt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m ).reset_index()\n\u001b[32m     21\u001b[39m stats[\u001b[33m'\u001b[39m\u001b[33mmodelo\u001b[39m\u001b[33m'\u001b[39m] = model_name\n",
      "\u001b[31mTypeError\u001b[39m: processar_dataframe() missing 2 required positional arguments: 'versao' and 'modelo_base'"
     ]
    }
   ],
   "source": [
    "\n",
    "resultados_estatisticas = []\n",
    "\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith('.csv') and filename in models_map:\n",
    "        model_name = models_map[filename]\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        print(f\"Processando: {model_name}\")\n",
    "        \n",
    "        df = processar_dataframe(file_path)\n",
    "        \n",
    "        stats = df.groupby('is_error')[['len_prompt', 'len_output']].agg(\n",
    "            len_prompt_mean=('len_prompt', 'mean'),\n",
    "            len_prompt_median=('len_prompt', 'median'),\n",
    "            len_prompt_std=('len_prompt', 'std'),\n",
    "            len_output_mean=('len_output', 'mean'),\n",
    "            len_output_median=('len_output', 'median'),\n",
    "            len_output_std=('len_output', 'std'),\n",
    "            total=('len_prompt', 'count')\n",
    "        ).reset_index()\n",
    "        \n",
    "        stats['modelo'] = model_name\n",
    "        resultados_estatisticas.append(stats)\n",
    "\n",
    "\n",
    "df_estatisticas = pd.concat(resultados_estatisticas, ignore_index=True)\n",
    "cols = ['modelo', 'is_error'] + [col for col in df_estatisticas.columns if col not in ['modelo', 'is_error']]\n",
    "df_estatisticas = df_estatisticas[cols]\n",
    "#df_estatisticas.to_csv('estatisticas_erros_modelos.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ccfa5",
   "metadata": {},
   "source": [
    "Acertos x Erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8775756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>is_error</th>\n",
       "      <th>len_prompt_mean</th>\n",
       "      <th>len_prompt_median</th>\n",
       "      <th>len_prompt_std</th>\n",
       "      <th>len_output_mean</th>\n",
       "      <th>len_output_median</th>\n",
       "      <th>len_output_std</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen2.5-7B</td>\n",
       "      <td>0</td>\n",
       "      <td>505.291798</td>\n",
       "      <td>423.0</td>\n",
       "      <td>251.167655</td>\n",
       "      <td>516.436122</td>\n",
       "      <td>434.0</td>\n",
       "      <td>251.195772</td>\n",
       "      <td>7303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen2.5-7B</td>\n",
       "      <td>1</td>\n",
       "      <td>529.720408</td>\n",
       "      <td>422.0</td>\n",
       "      <td>300.767668</td>\n",
       "      <td>535.435136</td>\n",
       "      <td>433.0</td>\n",
       "      <td>275.446209</td>\n",
       "      <td>7747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>0</td>\n",
       "      <td>506.782668</td>\n",
       "      <td>426.0</td>\n",
       "      <td>248.496444</td>\n",
       "      <td>518.537935</td>\n",
       "      <td>438.0</td>\n",
       "      <td>248.627879</td>\n",
       "      <td>5839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>1</td>\n",
       "      <td>524.892628</td>\n",
       "      <td>420.0</td>\n",
       "      <td>295.079523</td>\n",
       "      <td>531.713278</td>\n",
       "      <td>432.0</td>\n",
       "      <td>273.790782</td>\n",
       "      <td>9211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zephyr-7B</td>\n",
       "      <td>0</td>\n",
       "      <td>516.122513</td>\n",
       "      <td>427.0</td>\n",
       "      <td>271.970940</td>\n",
       "      <td>516.237238</td>\n",
       "      <td>434.0</td>\n",
       "      <td>244.200520</td>\n",
       "      <td>5779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         modelo  is_error  len_prompt_mean  len_prompt_median  len_prompt_std  \\\n",
       "0    Qwen2.5-7B         0       505.291798              423.0      251.167655   \n",
       "1    Qwen2.5-7B         1       529.720408              422.0      300.767668   \n",
       "2  Llama-3.2-3B         0       506.782668              426.0      248.496444   \n",
       "3  Llama-3.2-3B         1       524.892628              420.0      295.079523   \n",
       "4     Zephyr-7B         0       516.122513              427.0      271.970940   \n",
       "\n",
       "   len_output_mean  len_output_median  len_output_std  total  \n",
       "0       516.436122              434.0      251.195772   7303  \n",
       "1       535.435136              433.0      275.446209   7747  \n",
       "2       518.537935              438.0      248.627879   5839  \n",
       "3       531.713278              432.0      273.790782   9211  \n",
       "4       516.237238              434.0      244.200520   5779  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_estatisticas = pd.read_csv('estatisticas_erros_modelos.csv')\n",
    "df_estatisticas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010acb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo: /home/annap/Downloads/01_plots_html/acertos_erros_Llama-3.1-8B.html\n",
      "Salvo: /home/annap/Downloads/01_plots_html/acertos_erros_Llama-3.2-3B.html\n",
      "Salvo: /home/annap/Downloads/01_plots_html/acertos_erros_Qwen2.5-7B.html\n",
      "Salvo: /home/annap/Downloads/01_plots_html/acertos_erros_Qwen2.5-14B.html\n",
      "Salvo: /home/annap/Downloads/01_plots_html/acertos_erros_Phi-4-14B.html\n",
      "Salvo: /home/annap/Downloads/01_plots_html/acertos_erros_Zephyr-7B.html\n"
     ]
    }
   ],
   "source": [
    "'''def processar_dataframe(filepath, versao, modelo_base, df_answer):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['output_model'] = df['output_model'].str.upper()\n",
    "    df['last_answer'] = df['output_model'].str.extract(r'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\s*(.*)', flags=re.IGNORECASE).fillna('invalid')\n",
    "    df['model_answer'] = df['last_answer'].apply(extrair_model_answer)\n",
    "    df['len_prompt'] = df['prompt'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df['len_output'] = df['output_model'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df = df.merge(df_answer, on='ID', how='left')\n",
    "    df['is_error'] = (df['model_answer'] != df['ANSWER']).astype(int)\n",
    "    df['versao'] = versao\n",
    "    df['modelo_base'] = modelo_base\n",
    "    return df  # ⬅️ Agora retorna todos (acertos + erros)\n",
    "\n",
    "\n",
    "def plot_violin_acertos_erros(df_model, modelo_base):\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Tamanho do Prompt\", \"Tamanho do Output\"],\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "\n",
    "    color_map = {0: \"#2ca02c\", 1: \"#d62728\"}  # verde para acerto, vermelho para erro\n",
    "\n",
    "    for col, idx in zip([\"len_prompt\", \"len_output\"], [1, 2]):\n",
    "        violin_fig = px.violin(\n",
    "            df_model,\n",
    "            y=col,\n",
    "            color=\"is_error\",\n",
    "            box=True,\n",
    "            points=\"all\",\n",
    "            color_discrete_map=color_map,\n",
    "            category_orders={\"is_error\": [0, 1]},\n",
    "            labels={\"is_error\": \"Erro\"}\n",
    "        )\n",
    "        for trace in violin_fig.data:\n",
    "            fig.add_trace(trace, row=1, col=idx)\n",
    "\n",
    "        fig.update_yaxes(title_text=\"Tamanho\", row=1, col=idx)\n",
    "        fig.update_xaxes(title_text=\"\", row=1, col=idx, showticklabels=False)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Acertos vs. Erros - {modelo_base}\",\n",
    "        font=dict(size=14),\n",
    "        height=500,\n",
    "        width=1000,\n",
    "        plot_bgcolor=\"#f9f9f9\",\n",
    "        paper_bgcolor=\"white\",\n",
    "        showlegend=True,\n",
    "        legend=dict(title=\"Acerto (0) / Erro (1)\", orientation=\"h\", y=-0.2, x=0.25)\n",
    "    )\n",
    "\n",
    "    output_path = f\"/home/annap/Downloads/01_plots_html/acertos_erros_{modelo_base}.html\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    fig.write_html(output_path)\n",
    "    print(f\"Salvo: {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Loop principal\n",
    "for modelo_base, (v0_file, v1_file) in modelos_base.items():\n",
    "    dfs = []\n",
    "    v0_path = os.path.join(v0_dir, v0_file)\n",
    "    v1_path = os.path.join(v1_dir, v1_file)\n",
    "\n",
    "    if os.path.exists(v0_path):\n",
    "        dfs.append(processar_dataframe(v0_path, 'V0', modelo_base, df_answer))\n",
    "    if os.path.exists(v1_path):\n",
    "        dfs.append(processar_dataframe(v1_path, 'V1', modelo_base, df_answer))\n",
    "\n",
    "    if dfs:\n",
    "        df_model = pd.concat(dfs, ignore_index=True)\n",
    "        plot_violin_acertos_erros(df_model, modelo_base)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be95a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/dist_Llama-3.1-8B_V0_V1_violin.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/dist_Llama-3.2-3B_V0_V1_violin.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/dist_phi-4-14B_V0_V1_violin.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/dist_Zephyr-7B_V0_V1_violin.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/dist_Qwen2.5-7B_V0_V1_violin.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/dist_Qwen2.5-14B_V0_V1_violin.html\n"
     ]
    }
   ],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "\n",
    "def salvar_html(fig, caminho):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(caminho), exist_ok=True)\n",
    "        fig.write_html(caminho)\n",
    "        print(f\"Salvo HTML em: {caminho}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar HTML: {e}\")\n",
    "\n",
    "def extrair_model_answer(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return 'invalid'\n",
    "    match = re.search(r'[ABCD]', texto)\n",
    "    return match.group(0) if match else 'invalid'\n",
    "\n",
    "def processar_dataframe(filepath, versao):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['output_model'] = df['output_model'].str.upper()\n",
    "    df['last_answer'] = df['output_model'].str.extract(\n",
    "        r'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\s*(.*)', flags=re.IGNORECASE\n",
    "    ).fillna('invalid')\n",
    "    df['model_answer'] = df['last_answer'].apply(extrair_model_answer)\n",
    "    df['len_prompt'] = df['prompt'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df['len_output'] = df['output_model'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df = df.merge(df_answer, on='ID', how='left')\n",
    "    df['is_error'] = (df['model_answer'] != df['ANSWER']).astype(int)\n",
    "    df['resultado'] = df['is_error'].map({0: 'Acerto', 1: 'Erro'})\n",
    "    df['versao'] = versao\n",
    "    df['grupo'] = df['versao'] + \" - \" + df['resultado']\n",
    "    return df\n",
    "\n",
    "def criar_violin_plot(df, coluna, row, fig):\n",
    "    color_map = {\n",
    "        \"V0 - Acerto\": \"#4c78a8\",  # Azul claro\n",
    "        \"V1 - Acerto\": \"#6baed6\",  # Azul mais claro\n",
    "        \"V0 - Erro\": \"#e45756\",    # Vermelho vivo\n",
    "        \"V1 - Erro\": \"#f28e2b\",    # Vermelho alaranjado\n",
    "    }\n",
    "    violin = px.violin(\n",
    "        df,\n",
    "        y=coluna,\n",
    "        x=\"grupo\",\n",
    "        color=\"grupo\",\n",
    "        box=True,\n",
    "        points=\"all\",\n",
    "        color_discrete_map=color_map\n",
    "    )\n",
    "    for trace in violin.data:\n",
    "        fig.add_trace(trace, row=row, col=1)\n",
    "\n",
    "    fig.update_yaxes(title_text=\"Tamanho\", row=row, col=1)\n",
    "    fig.update_xaxes(title_text=\"\", row=row, col=1, showticklabels=True)\n",
    "\n",
    "def plot_violin_duplo(df, model_name):\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        shared_xaxes=False,  # Importante: não compartilhar eixo x se queremos exibir ticks em ambos\n",
    "        vertical_spacing=0.1,\n",
    "        subplot_titles=[\n",
    "            \"Distribuição do Tamanho do Prompt de Entrada\",\n",
    "            \"Distribuição do Tamanho do Prompt de Saída do Modelo\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    criar_violin_plot(df, \"len_prompt\", row=1, fig=fig)\n",
    "    criar_violin_plot(df, \"len_output\", row=2, fig=fig)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=850,\n",
    "        width=1000,\n",
    "        title_text=f\"Distribuições por Versão e Resultado - {model_name}\",\n",
    "        showlegend=False,\n",
    "        font=dict(color=\"black\", size=16),\n",
    "        plot_bgcolor=\"#f9f9f9\",\n",
    "        paper_bgcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    salvar_html(fig, f\"/home/annap/Downloads/01_plots_html/dist_{model_name}_V0_V1_violin.html\")\n",
    "\n",
    "# Caminhos\n",
    "v0_dir = '../UNSLOTH/NO_FINE_TUNE/GLOSA_PT'\n",
    "v1_dir = '../UNSLOTH/V1/GLOSA_PT'\n",
    "answer_path = '/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv'\n",
    "\n",
    "# Mapeamento dos modelos\n",
    "modelos_base = {\n",
    "    \"Llama-3.1-8B\": [\"Llama-3.1-8B-unsloth-bnb-4bit.csv\", \"Llama-3.1-8B-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Llama-3.2-3B\": [\"Llama-3.2-3B-Instruct-unsloth-bnb-4bit.csv\", \"Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"phi-4-14B\": [\"phi-4-unsloth-bnb-4bit.csv\", \"phi-4-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Zephyr-7B\": [\"zephyr-sft-bnb-4bit.csv\", \"zephyr-sft-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-7B\": [\"Qwen2.5-7B-Instruct-bnb-4bit.csv\", \"Qwen2.5-7B-Instruct-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-14B\": [\"Qwen2.5-14B-Instruct-unsloth-bnb-4bit.csv\", \"Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "}\n",
    "\n",
    "# Respostas\n",
    "df_answer = pd.read_csv(answer_path)[['ID', 'ANSWER']]\n",
    "df_answer['ANSWER'] = df_answer['ANSWER'].map({0: 'A', 1: 'B', 2: 'C', 3: 'D'})\n",
    "\n",
    "# Loop principal\n",
    "for model_name, (v0_file, v1_file) in modelos_base.items():\n",
    "    v0_path = os.path.join(v0_dir, v0_file)\n",
    "    v1_path = os.path.join(v1_dir, v1_file)\n",
    "\n",
    "    if not os.path.exists(v0_path) or not os.path.exists(v1_path):\n",
    "        print(f\"[!] Arquivo ausente para {model_name}, pulando.\")\n",
    "        continue\n",
    "\n",
    "    df_v0 = processar_dataframe(v0_path, \"V0\")\n",
    "    df_v1 = processar_dataframe(v1_path, \"V1\")\n",
    "    df_combined = pd.concat([df_v0, df_v1], ignore_index=True)\n",
    "\n",
    "    plot_violin_duplo(df_combined, model_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6ebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_Llama-3.1-8B_V0_V1.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_Llama-3.2-3B_V0_V1.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_phi-4-14B_V0_V1.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_Zephyr-7B_V0_V1.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_Qwen2.5-7B_V0_V1.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_Qwen2.5-14B_V0_V1.html\n"
     ]
    }
   ],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "\n",
    "def salvar_html(fig, caminho):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(caminho), exist_ok=True)\n",
    "        fig.write_html(caminho)\n",
    "        print(f\"Salvo HTML em: {caminho}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar HTML: {e}\")\n",
    "\n",
    "def extrair_model_answer(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return 'invalid'\n",
    "    match = re.search(r'[ABCD]', texto)\n",
    "    return match.group(0) if match else 'invalid'\n",
    "\n",
    "def processar_dataframe(filepath, versao):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['output_model'] = df['output_model'].str.upper()\n",
    "    df['last_answer'] = df['output_model'].str.extract(\n",
    "        r'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\s*(.*)', flags=re.IGNORECASE\n",
    "    ).fillna('invalid')\n",
    "    df['model_answer'] = df['last_answer'].apply(extrair_model_answer)\n",
    "    df['len_prompt'] = df['prompt'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df['len_output'] = df['output_model'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df = df.merge(df_answer, on='ID', how='left')\n",
    "    df['is_error'] = (df['model_answer'] != df['ANSWER']).astype(int)\n",
    "    df['resultado'] = df['is_error'].map({0: 'Acerto', 1: 'Erro'})\n",
    "    df['versao'] = versao\n",
    "    df['grupo'] = df['versao'] + \" - \" + df['resultado']\n",
    "    return df\n",
    "\n",
    "def criar_box_plot(df, coluna, row, fig):\n",
    "    color_map = {\n",
    "        \"V0 - Acerto\": \"#4c78a8\",\n",
    "        \"V1 - Acerto\": \"#6baed6\",\n",
    "        \"V0 - Erro\": \"#e45756\",\n",
    "        \"V1 - Erro\": \"#f28e2b\",\n",
    "    }\n",
    "    box = px.box(\n",
    "        df,\n",
    "        y=coluna,\n",
    "        x=\"grupo\",\n",
    "        color=\"grupo\",\n",
    "        points=\"all\",\n",
    "        color_discrete_map=color_map\n",
    "    )\n",
    "    for trace in box.data:\n",
    "        fig.add_trace(trace, row=row, col=1)\n",
    "\n",
    "    fig.update_yaxes(title_text=\"Tamanho\", row=row, col=1)\n",
    "    fig.update_xaxes(title_text=\"\", row=row, col=1, showticklabels=True)\n",
    "\n",
    "def plot_box_duplo(df, model_name):\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        shared_xaxes=False,\n",
    "        vertical_spacing=0.1,\n",
    "        subplot_titles=[\n",
    "            \"Distribuição do Tamanho do Prompt de Entrada\",\n",
    "            \"Distribuição do Tamanho do Prompt de Saída do Modelo\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    criar_box_plot(df, \"len_prompt\", row=1, fig=fig)\n",
    "    criar_box_plot(df, \"len_output\", row=2, fig=fig)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=850,\n",
    "        width=1000,\n",
    "        title_text=f\"Distribuições por Versão e Resultado (Box Plot) - {model_name}\",\n",
    "        showlegend=False,\n",
    "        font=dict(color=\"black\", size=16),\n",
    "        plot_bgcolor=\"#f9f9f9\",\n",
    "        paper_bgcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    salvar_html(fig, f\"/home/annap/Downloads/01_plots_html/box_{model_name}_V0_V1.html\")\n",
    "\n",
    "# Caminhos\n",
    "v0_dir = '../UNSLOTH/NO_FINE_TUNE/GLOSA_PT'\n",
    "v1_dir = '../UNSLOTH/V1/GLOSA_PT'\n",
    "answer_path = '/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv'\n",
    "\n",
    "# Mapeamento dos modelos\n",
    "modelos_base = {\n",
    "    \"Llama-3.1-8B\": [\"Llama-3.1-8B-unsloth-bnb-4bit.csv\", \"Llama-3.1-8B-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Llama-3.2-3B\": [\"Llama-3.2-3B-Instruct-unsloth-bnb-4bit.csv\", \"Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"phi-4-14B\": [\"phi-4-unsloth-bnb-4bit.csv\", \"phi-4-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Zephyr-7B\": [\"zephyr-sft-bnb-4bit.csv\", \"zephyr-sft-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-7B\": [\"Qwen2.5-7B-Instruct-bnb-4bit.csv\", \"Qwen2.5-7B-Instruct-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-14B\": [\"Qwen2.5-14B-Instruct-unsloth-bnb-4bit.csv\", \"Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "}\n",
    "\n",
    "# Respostas\n",
    "df_answer = pd.read_csv(answer_path)[['ID', 'ANSWER']]\n",
    "df_answer['ANSWER'] = df_answer['ANSWER'].map({0: 'A', 1: 'B', 2: 'C', 3: 'D'})\n",
    "\n",
    "# Loop principal\n",
    "for model_name, (v0_file, v1_file) in modelos_base.items():\n",
    "    v0_path = os.path.join(v0_dir, v0_file)\n",
    "    v1_path = os.path.join(v1_dir, v1_file)\n",
    "\n",
    "    if not os.path.exists(v0_path) or not os.path.exists(v1_path):\n",
    "        print(f\"[!] Arquivo ausente para {model_name}, pulando.\")\n",
    "        continue\n",
    "\n",
    "    df_v0 = processar_dataframe(v0_path, \"V0\")\n",
    "    df_v1 = processar_dataframe(v1_path, \"V1\")\n",
    "    df_combined = pd.concat([df_v0, df_v1], ignore_index=True)\n",
    "\n",
    "    plot_box_duplo(df_combined, model_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4268a882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_Llama-3.1-8B_V0_V1.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_Llama-3.2-3B_V0_V1.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_phi-4-14B_V0_V1.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_Zephyr-7B_V0_V1.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_Qwen2.5-7B_V0_V1.html\n",
      "Salvo HTML em: /home/annap/Downloads/01_plots_html/box_Qwen2.5-14B_V0_V1.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "\n",
    "def salvar_html(fig, caminho):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(caminho), exist_ok=True)\n",
    "        fig.write_html(caminho)\n",
    "        print(f\"Salvo HTML em: {caminho}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar HTML: {e}\")\n",
    "\n",
    "def extrair_model_answer(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return 'invalid'\n",
    "    match = re.search(r'[ABCD]', texto)\n",
    "    return match.group(0) if match else 'invalid'\n",
    "\n",
    "def processar_dataframe(filepath, versao):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['output_model'] = df['output_model'].str.upper()\n",
    "    df['last_answer'] = df['output_model'].str.extract(\n",
    "        r'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\s*(.*)', flags=re.IGNORECASE\n",
    "    ).fillna('invalid')\n",
    "    df['model_answer'] = df['last_answer'].apply(extrair_model_answer)\n",
    "    df['len_prompt'] = df['prompt'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df['len_output'] = df['output_model'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df = df.merge(df_answer, on='ID', how='left')\n",
    "    df['is_error'] = (df['model_answer'] != df['ANSWER']).astype(int)\n",
    "    df['resultado'] = df['is_error'].map({0: 'Acerto', 1: 'Erro'})\n",
    "    df['versao'] = versao\n",
    "    df['grupo'] = df['versao'] + \" - \" + df['resultado']\n",
    "    return df\n",
    "\n",
    "def criar_box_plot(df, coluna, row, fig):\n",
    "    ordem_grupo = [\"V0 - Erro\", \"V0 - Acerto\", \"V1 - Acerto\", \"V1 - Erro\"]\n",
    "    color_map = {\n",
    "        \"V0 - Erro\": \"#e45756\",\n",
    "        \"V0 - Acerto\": \"#4c78a8\",\n",
    "        \"V1 - Acerto\": \"#6baed6\",\n",
    "        \"V1 - Erro\": \"#f22b67\",\n",
    "    }\n",
    "\n",
    "    box = px.box(\n",
    "        df,\n",
    "        y=coluna,\n",
    "        x=\"grupo\",\n",
    "        color=\"grupo\",\n",
    "        points=\"all\",\n",
    "        color_discrete_map=color_map,\n",
    "        category_orders={\"grupo\": ordem_grupo}\n",
    "    )\n",
    "    for trace in box.data:\n",
    "        fig.add_trace(trace, row=row, col=1)\n",
    "\n",
    "    fig.update_yaxes(title_text=\"Tamanho\", row=row, col=1)\n",
    "    fig.update_xaxes(title_text=\"\", row=row, col=1, showticklabels=True)\n",
    "\n",
    "\n",
    "def plot_box_duplo(df, model_name):\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        shared_xaxes=False,\n",
    "        vertical_spacing=0.1,\n",
    "        subplot_titles=[\n",
    "            \"Distribuição do Tamanho do Prompt de Entrada do Modelo\",\n",
    "            \"Distribuição do Tamanho do Prompt de Saída do Modelo\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    criar_box_plot(df, \"len_prompt\", row=1, fig=fig)\n",
    "    criar_box_plot(df, \"len_output\", row=2, fig=fig)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=850,\n",
    "        width=1000,\n",
    "        title_text=f\"Distribuições por Versão e Resultado (Box Plot) - {model_name}\",\n",
    "        showlegend=False,\n",
    "        font=dict(color=\"black\", size=16),\n",
    "        plot_bgcolor=\"#f9f9f9\",\n",
    "        paper_bgcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    salvar_html(fig, f\"/home/annap/Downloads/01_plots_html/box_{model_name}_V0_V1.html\")\n",
    "\n",
    "# Caminhos\n",
    "v0_dir = '../UNSLOTH/NO_FINE_TUNE/GLOSA_PT'\n",
    "v1_dir = '../UNSLOTH/V1/GLOSA_PT'\n",
    "answer_path = '/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv'\n",
    "\n",
    "# Mapeamento dos modelos\n",
    "modelos_base = {\n",
    "    \"Llama-3.1-8B\": [\"Llama-3.1-8B-unsloth-bnb-4bit.csv\", \"Llama-3.1-8B-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Llama-3.2-3B\": [\"Llama-3.2-3B-Instruct-unsloth-bnb-4bit.csv\", \"Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"phi-4-14B\": [\"phi-4-unsloth-bnb-4bit.csv\", \"phi-4-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Zephyr-7B\": [\"zephyr-sft-bnb-4bit.csv\", \"zephyr-sft-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-7B\": [\"Qwen2.5-7B-Instruct-bnb-4bit.csv\", \"Qwen2.5-7B-Instruct-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-14B\": [\"Qwen2.5-14B-Instruct-unsloth-bnb-4bit.csv\", \"Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "}\n",
    "\n",
    "# Respostas\n",
    "df_answer = pd.read_csv(answer_path)[['ID', 'ANSWER']]\n",
    "df_answer['ANSWER'] = df_answer['ANSWER'].map({0: 'A', 1: 'B', 2: 'C', 3: 'D'})\n",
    "\n",
    "# Loop principal\n",
    "for model_name, (v0_file, v1_file) in modelos_base.items():\n",
    "    v0_path = os.path.join(v0_dir, v0_file)\n",
    "    v1_path = os.path.join(v1_dir, v1_file)\n",
    "\n",
    "    if not os.path.exists(v0_path) or not os.path.exists(v1_path):\n",
    "        print(f\"[!] Arquivo ausente para {model_name}, pulando.\")\n",
    "        continue\n",
    "\n",
    "    df_v0 = processar_dataframe(v0_path, \"V0\")\n",
    "    df_v1 = processar_dataframe(v1_path, \"V1\")\n",
    "    df_combined = pd.concat([df_v0, df_v1], ignore_index=True)\n",
    "\n",
    "    plot_box_duplo(df_combined, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432ee30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "714ecb4a",
   "metadata": {},
   "source": [
    "# Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb4e8203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ANSWER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID ANSWER\n",
       "0   0      B\n",
       "1   1      C\n",
       "2   2      D\n",
       "3   3      B\n",
       "4   4      B"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = '../UNSLOTH/V1/GLOSA_PT'\n",
    "answer_path = '/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv'\n",
    "\n",
    "models_map = {\n",
    "    'Llama-3.1-8B-unsloth-bnb-4bit-V1.csv': 'Llama-3.1-8B-V1',\n",
    "    'Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv': 'Llama-3.2-3B-V1',\n",
    "    'phi-4-unsloth-bnb-4bit-V1.csv': 'Phi-4-14B-V1',\n",
    "    'zephyr-sft-bnb-4bit-V1.csv': 'Zephyr-7B-V1',\n",
    "    'Qwen2.5-7B-Instruct-bnb-4bit-V1.csv': 'Qwen2.5-7B-V1',\n",
    "    'Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv': 'Qwen2.5-14B-V1',\n",
    "}\n",
    "\n",
    "df_answer = pd.read_csv(answer_path)[['ID', 'ANSWER']]\n",
    "answer_mapping = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}\n",
    "df_answer['ANSWER'] = df_answer['ANSWER'].map(answer_mapping)\n",
    "df_answer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       subject  Llama-3.1-8B-V0  Llama-3.1-8B-V1  \\\n",
      "0             professional_law             1390             1171   \n",
      "1              moral_scenarios              983              728   \n",
      "2                miscellaneous              579              454   \n",
      "3      professional_psychology              574              409   \n",
      "4       high_school_psychology              464              302   \n",
      "5   high_school_macroeconomics              392              268   \n",
      "6       elementary_mathematics              366              262   \n",
      "7               moral_disputes              319              235   \n",
      "8                   philosophy              300              205   \n",
      "9      professional_accounting              292              212   \n",
      "10                  prehistory                0                0   \n",
      "11     high_school_mathematics                0                0   \n",
      "\n",
      "    Llama-3.2-3B-V0  Llama-3.2-3B-V1  Qwen2.5-14B-V0  Qwen2.5-14B-V1  \\\n",
      "0              1166             1140            1109            1058   \n",
      "1               721              714             722             708   \n",
      "2               480              484             339             308   \n",
      "3               424              419             337             306   \n",
      "4               308              311             222             168   \n",
      "5               280              296             200             158   \n",
      "6               250              275             210             166   \n",
      "7               224              221             207             183   \n",
      "8               201              202             162             144   \n",
      "9               216              213             194             183   \n",
      "10                0                0               0               0   \n",
      "11                0                0               0               0   \n",
      "\n",
      "    Qwen2.5-7B-V0  Qwen2.5-7B-V1  Zephyr-7B-V0  Zephyr-7B-V1  phi-4-14B-V0  \\\n",
      "0            1095           1094          1128          1139          1666   \n",
      "1             726            744           753           737           995   \n",
      "2             360            345           457           451           865   \n",
      "3             359            345           448           443           667   \n",
      "4             231            209           306           304           601   \n",
      "5             206            193           265           254           431   \n",
      "6             208            193           257           256           380   \n",
      "7             194            196           238           242           383   \n",
      "8               0              0           225           204             0   \n",
      "9             201            196           216           223           311   \n",
      "10              0              0             0             0           359   \n",
      "11            161            164             0             0             0   \n",
      "\n",
      "    phi-4-14B-V1  \n",
      "0           1026  \n",
      "1            741  \n",
      "2            308  \n",
      "3            317  \n",
      "4            195  \n",
      "5            177  \n",
      "6            184  \n",
      "7            170  \n",
      "8              0  \n",
      "9            190  \n",
      "10           136  \n",
      "11             0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def processar_dataframe(filepath, versao):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['output_model'] = df['output_model'].str.upper()\n",
    "    df['last_answer'] = df['output_model'].str.extract(\n",
    "        r'(?:ASSISTANT:|ANSWER:|RESPOSTA:)\\s*(.*)', flags=re.IGNORECASE\n",
    "    ).fillna('invalid')\n",
    "    df['model_answer'] = df['last_answer'].apply(lambda x: re.search(r'[ABCD]', x).group(0) if isinstance(x, str) and re.search(r'[ABCD]', x) else 'invalid')\n",
    "    df['len_prompt'] = df['prompt'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df['len_output'] = df['output_model'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    df = df.merge(df_answer, on='ID', how='left')\n",
    "    df['is_error'] = (df['model_answer'] != df['ANSWER']).astype(int)\n",
    "    df['resultado'] = df['is_error'].map({0: 'Acerto', 1: 'Erro'})\n",
    "    df['versao'] = versao\n",
    "    df['grupo'] = df['versao'] + \" - \" + df['resultado']\n",
    "    return df\n",
    "\n",
    "def gerar_tabela_subject_erros(modelos_base, v0_dir, v1_dir, df_answer_base, subject_path):\n",
    "    # Carrega os assuntos por questão\n",
    "    subject_df = pd.read_csv(subject_path)[['question_id', 'subject']]\n",
    "\n",
    "    resultados = {}\n",
    "\n",
    "    for model_name, (v0_file, v1_file) in modelos_base.items():\n",
    "        v0_path = os.path.join(v0_dir, v0_file)\n",
    "        v1_path = os.path.join(v1_dir, v1_file)\n",
    "\n",
    "        if not os.path.exists(v0_path) or not os.path.exists(v1_path):\n",
    "            print(f\"[!] Arquivo ausente para {model_name}, pulando.\")\n",
    "            continue\n",
    "\n",
    "        # Merge dos assuntos com as respostas esperadas\n",
    "        df_answer = df_answer_base.copy()\n",
    "        df_answer = df_answer.merge(subject_df, left_on='ID', right_on='question_id', how='left')\n",
    "\n",
    "        # Processa os dataframes das duas versões\n",
    "        df_v0 = processar_dataframe(v0_path, \"V0\").merge(subject_df, left_on='ID', right_on='question_id', how='left')\n",
    "        df_v1 = processar_dataframe(v1_path, \"V1\").merge(subject_df, left_on='ID', right_on='question_id', how='left')\n",
    "\n",
    "        # Filtra apenas os erros\n",
    "        erros_v0 = df_v0[df_v0['is_error'] == 1]['subject'].value_counts()\n",
    "        erros_v1 = df_v1[df_v1['is_error'] == 1]['subject'].value_counts()\n",
    "\n",
    "        # Seleciona os 10 assuntos mais problemáticos (soma total dos dois)\n",
    "        top_subjects = (erros_v0.add(erros_v1, fill_value=0)).sort_values(ascending=False).head(10).index\n",
    "\n",
    "        for subject in top_subjects:\n",
    "            chave_v0 = f\"{model_name}-V0\"\n",
    "            chave_v1 = f\"{model_name}-V1\"\n",
    "\n",
    "            if subject not in resultados:\n",
    "                resultados[subject] = {}\n",
    "\n",
    "            resultados[subject][chave_v0] = int(erros_v0.get(subject, 0))\n",
    "            resultados[subject][chave_v1] = int(erros_v1.get(subject, 0))\n",
    "\n",
    "    # Gera o DataFrame final com os resultados\n",
    "    df_result = pd.DataFrame(resultados).T.fillna(0).astype(int)\n",
    "    df_result = df_result.sort_index(axis=1)  # ordena colunas alfabeticamente\n",
    "    df_result.reset_index(inplace=True)\n",
    "    df_result.rename(columns={'index': 'subject'}, inplace=True)\n",
    "\n",
    "    return df_result\n",
    "\n",
    "# Caminhos\n",
    "v0_dir = '../UNSLOTH/NO_FINE_TUNE/GLOSA_PT'\n",
    "v1_dir = '../UNSLOTH/V1/GLOSA_PT'\n",
    "answer_path = '/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv'\n",
    "subject_path = \"/home/annap/Documentos/chatbot_copy/DATASETS/MMLU/0_mmlu.csv\"\n",
    "\n",
    "# Mapeamento dos modelos\n",
    "modelos_base = {\n",
    "    \"Llama-3.1-8B\": [\"Llama-3.1-8B-unsloth-bnb-4bit.csv\", \"Llama-3.1-8B-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Llama-3.2-3B\": [\"Llama-3.2-3B-Instruct-unsloth-bnb-4bit.csv\", \"Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"phi-4-14B\": [\"phi-4-unsloth-bnb-4bit.csv\", \"phi-4-unsloth-bnb-4bit-V1.csv\"],\n",
    "    \"Zephyr-7B\": [\"zephyr-sft-bnb-4bit.csv\", \"zephyr-sft-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-7B\": [\"Qwen2.5-7B-Instruct-bnb-4bit.csv\", \"Qwen2.5-7B-Instruct-bnb-4bit-V1.csv\"],\n",
    "    \"Qwen2.5-14B\": [\"Qwen2.5-14B-Instruct-unsloth-bnb-4bit.csv\", \"Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V1.csv\"],\n",
    "}\n",
    "\n",
    "# Respostas\n",
    "df_answer = pd.read_csv(answer_path)[['ID', 'ANSWER']]\n",
    "df_answer['ANSWER'] = df_answer['ANSWER'].map({0: 'A', 1: 'B', 2: 'C', 3: 'D'})\n",
    "\n",
    "# Gera a tabela\n",
    "tabela_subject_erro = gerar_tabela_subject_erros(\n",
    "    modelos_base, v0_dir, v1_dir, df_answer, subject_path\n",
    ")\n",
    "\n",
    "print(tabela_subject_erro)\n",
    "\n",
    "# (Opcional) Salvar em CSV\n",
    "tabela_subject_erro.to_csv(\"tabela_subject_erro.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "051d00b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Llama-3.1-8B-V0</th>\n",
       "      <th>Llama-3.1-8B-V1</th>\n",
       "      <th>Llama-3.2-3B-V0</th>\n",
       "      <th>Llama-3.2-3B-V1</th>\n",
       "      <th>Qwen2.5-14B-V0</th>\n",
       "      <th>Qwen2.5-14B-V1</th>\n",
       "      <th>Qwen2.5-7B-V0</th>\n",
       "      <th>Qwen2.5-7B-V1</th>\n",
       "      <th>Zephyr-7B-V0</th>\n",
       "      <th>Zephyr-7B-V1</th>\n",
       "      <th>phi-4-14B-V0</th>\n",
       "      <th>phi-4-14B-V1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>professional_law</td>\n",
       "      <td>1390</td>\n",
       "      <td>1171</td>\n",
       "      <td>1166</td>\n",
       "      <td>1140</td>\n",
       "      <td>1109</td>\n",
       "      <td>1058</td>\n",
       "      <td>1095</td>\n",
       "      <td>1094</td>\n",
       "      <td>1128</td>\n",
       "      <td>1139</td>\n",
       "      <td>1666</td>\n",
       "      <td>1026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>moral_scenarios</td>\n",
       "      <td>983</td>\n",
       "      <td>728</td>\n",
       "      <td>721</td>\n",
       "      <td>714</td>\n",
       "      <td>722</td>\n",
       "      <td>708</td>\n",
       "      <td>726</td>\n",
       "      <td>744</td>\n",
       "      <td>753</td>\n",
       "      <td>737</td>\n",
       "      <td>995</td>\n",
       "      <td>741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>579</td>\n",
       "      <td>454</td>\n",
       "      <td>480</td>\n",
       "      <td>484</td>\n",
       "      <td>339</td>\n",
       "      <td>308</td>\n",
       "      <td>360</td>\n",
       "      <td>345</td>\n",
       "      <td>457</td>\n",
       "      <td>451</td>\n",
       "      <td>865</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>professional_psychology</td>\n",
       "      <td>574</td>\n",
       "      <td>409</td>\n",
       "      <td>424</td>\n",
       "      <td>419</td>\n",
       "      <td>337</td>\n",
       "      <td>306</td>\n",
       "      <td>359</td>\n",
       "      <td>345</td>\n",
       "      <td>448</td>\n",
       "      <td>443</td>\n",
       "      <td>667</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high_school_psychology</td>\n",
       "      <td>464</td>\n",
       "      <td>302</td>\n",
       "      <td>308</td>\n",
       "      <td>311</td>\n",
       "      <td>222</td>\n",
       "      <td>168</td>\n",
       "      <td>231</td>\n",
       "      <td>209</td>\n",
       "      <td>306</td>\n",
       "      <td>304</td>\n",
       "      <td>601</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>high_school_macroeconomics</td>\n",
       "      <td>392</td>\n",
       "      <td>268</td>\n",
       "      <td>280</td>\n",
       "      <td>296</td>\n",
       "      <td>200</td>\n",
       "      <td>158</td>\n",
       "      <td>206</td>\n",
       "      <td>193</td>\n",
       "      <td>265</td>\n",
       "      <td>254</td>\n",
       "      <td>431</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>elementary_mathematics</td>\n",
       "      <td>366</td>\n",
       "      <td>262</td>\n",
       "      <td>250</td>\n",
       "      <td>275</td>\n",
       "      <td>210</td>\n",
       "      <td>166</td>\n",
       "      <td>208</td>\n",
       "      <td>193</td>\n",
       "      <td>257</td>\n",
       "      <td>256</td>\n",
       "      <td>380</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>moral_disputes</td>\n",
       "      <td>319</td>\n",
       "      <td>235</td>\n",
       "      <td>224</td>\n",
       "      <td>221</td>\n",
       "      <td>207</td>\n",
       "      <td>183</td>\n",
       "      <td>194</td>\n",
       "      <td>196</td>\n",
       "      <td>238</td>\n",
       "      <td>242</td>\n",
       "      <td>383</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>philosophy</td>\n",
       "      <td>300</td>\n",
       "      <td>205</td>\n",
       "      <td>201</td>\n",
       "      <td>202</td>\n",
       "      <td>162</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>225</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>professional_accounting</td>\n",
       "      <td>292</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "      <td>213</td>\n",
       "      <td>194</td>\n",
       "      <td>183</td>\n",
       "      <td>201</td>\n",
       "      <td>196</td>\n",
       "      <td>216</td>\n",
       "      <td>223</td>\n",
       "      <td>311</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      subject  Llama-3.1-8B-V0  Llama-3.1-8B-V1  \\\n",
       "0            professional_law             1390             1171   \n",
       "1             moral_scenarios              983              728   \n",
       "2               miscellaneous              579              454   \n",
       "3     professional_psychology              574              409   \n",
       "4      high_school_psychology              464              302   \n",
       "5  high_school_macroeconomics              392              268   \n",
       "6      elementary_mathematics              366              262   \n",
       "7              moral_disputes              319              235   \n",
       "8                  philosophy              300              205   \n",
       "9     professional_accounting              292              212   \n",
       "\n",
       "   Llama-3.2-3B-V0  Llama-3.2-3B-V1  Qwen2.5-14B-V0  Qwen2.5-14B-V1  \\\n",
       "0             1166             1140            1109            1058   \n",
       "1              721              714             722             708   \n",
       "2              480              484             339             308   \n",
       "3              424              419             337             306   \n",
       "4              308              311             222             168   \n",
       "5              280              296             200             158   \n",
       "6              250              275             210             166   \n",
       "7              224              221             207             183   \n",
       "8              201              202             162             144   \n",
       "9              216              213             194             183   \n",
       "\n",
       "   Qwen2.5-7B-V0  Qwen2.5-7B-V1  Zephyr-7B-V0  Zephyr-7B-V1  phi-4-14B-V0  \\\n",
       "0           1095           1094          1128          1139          1666   \n",
       "1            726            744           753           737           995   \n",
       "2            360            345           457           451           865   \n",
       "3            359            345           448           443           667   \n",
       "4            231            209           306           304           601   \n",
       "5            206            193           265           254           431   \n",
       "6            208            193           257           256           380   \n",
       "7            194            196           238           242           383   \n",
       "8              0              0           225           204             0   \n",
       "9            201            196           216           223           311   \n",
       "\n",
       "   phi-4-14B-V1  \n",
       "0          1026  \n",
       "1           741  \n",
       "2           308  \n",
       "3           317  \n",
       "4           195  \n",
       "5           177  \n",
       "6           184  \n",
       "7           170  \n",
       "8             0  \n",
       "9           190  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabela_subject_erro.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ed5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
