{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a375e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelos que contêm 'chatbot-' no nome:\n",
      "annagoncalves2/chatbot-gemma-3-12b-it-bnb-4bit-test\n",
      "annagoncalves2/chatbot-Llama-3.1-8B-unsloth-bnb-4bit-test\n",
      "annagoncalves2/chatbot-Llama-3.2-3B-Instruct-unsloth-bnb-4bit-test\n",
      "annagoncalves2/chatbot-phi-4-unsloth-bnb-4bit-test\n",
      "annagoncalves2/chatbot-Qwen2.5-7B-Instruct-bnb-4bit-test\n",
      "annagoncalves2/chatbot-Qwen2.5-14B-Instruct-unsloth-bnb-4bit-test\n",
      "annagoncalves2/chatbot-zephyr-sft-bnb-4bit-test\n",
      "annagoncalves2/chatbot-DeepSeek-R1-Distill-Llama-8B-test\n",
      "annagoncalves2/chatbot-Llama-3.1-8B-unsloth-bnb-4bit-V2\n",
      "annagoncalves2/chatbot-Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V2\n",
      "annagoncalves2/chatbot-phi-4-unsloth-bnb-4bit-V2\n",
      "annagoncalves2/chatbot-Qwen2.5-7B-Instruct-bnb-4bit-V2\n",
      "annagoncalves2/chatbot-Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V2\n",
      "annagoncalves2/chatbot-zephyr-sft-bnb-4bit-V2\n",
      "annagoncalves2/chatbot-DeepSeek-R1-Distill-Llama-8B-V2\n",
      "annagoncalves2/chatbot-gemma-3-12b-it-bnb-4bit-V2\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import delete_repo\n",
    "# Substitua pelo seu token\n",
    "token = \"\"\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Listar os modelos do autor (usuário ou organização)\n",
    "model_list = api.list_models(author=\"annagoncalves2\", token=token)\n",
    "\n",
    "# Exibir os modelos\n",
    "#print(\"Modelos (incluindo privados):\")\n",
    "#for model in model_list:\n",
    "#    print(model.modelId)\n",
    "\n",
    "print(\"\\nModelos que contêm 'chatbot-' no nome:\")\n",
    "for model in api.list_models(author=\"annagoncalves2\", token=token):\n",
    "    if \"chatbot-\" in model.modelId:\n",
    "        print(model.modelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afb995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deletado: annagoncalves2/glosa-Llama-3.2-1B-Instruct-bnb-4bit-gguf\n",
      "✅ Deletado: annagoncalves2/Llama-3.2-1B-Instruct-bnb-4bit-gguf\n",
      "✅ Deletado: annagoncalves2/DeepSeek-R1-Distill-Llama-8B-gguf\n",
      "✅ Deletado: annagoncalves2/Llama-3.3-70B-Instruct-bnb-4bit-gguf\n",
      "✅ Deletado: annagoncalves2/Qwen2.5-7B-Instruct-bnb-4bit-gguf\n"
     ]
    }
   ],
   "source": [
    "#ESSES AQUI TBM TEM QUE INFERIR\n",
    "repos = [\n",
    "    \"annagoncalves2/glosa-Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"annagoncalves2/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"annagoncalves2/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"annagoncalves2/Llama-3.3-70B-Instruct-bnb-4bit\",\n",
    "    \"annagoncalves2/Qwen2.5-7B-Instruct-bnb-4bit\",\n",
    "    \"annagoncalves2/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\",\n",
    "    \"annagoncalves2/Llama-3.2-3B-Instruct-unsloth-bnb-4bit-v2\",\n",
    "    \"annagoncalves2/Qwen2.5-7B-Instruct-bnb-4bit-v2\",\n",
    "    \"annagoncalves2/DeepSeek-R1-Distill-Llama-8B-v2\",\n",
    "    \"annagoncalves2/phi-4-unsloth-bnb-4bit-v2\",\n",
    "    \"annagoncalves2/zephyr-sft-bnb-4bit-v2\",\n",
    "    \"annagoncalves2/Llama-3.3-70B-Instruct-bnb-4bit-v2\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038c3c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annap/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[1;32m      2\u001b[0m api \u001b[38;5;241m=\u001b[39m HfApi()\n\u001b[0;32m----> 3\u001b[0m models \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mlist_models(author\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannagoncalves2\u001b[39m\u001b[38;5;124m\"\u001b[39m, token\u001b[38;5;241m=\u001b[39m\u001b[43mtoken\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(m\u001b[38;5;241m.\u001b[39mmodelId)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "models = api.list_models(author=\"annagoncalves2\", token=token)\n",
    "for m in models:\n",
    "    print(m.modelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224fd6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annap/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando texto...\n",
      "Memória GPU antes de limpar: 0.00 MB\n",
      "Memória GPU depois de limpar: 0.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annap/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:902: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/annap/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: {'input_ids': tensor([[    1,  7012, 28820, 28725,  4109,  1233,   264,   710,  3225,  2069,\n",
      "           511, 16386,  2646,  7463,  2099, 28804]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "Input IDs shape: torch.Size([1, 16])\n",
      "Outputs: tensor([[    1,  7012, 28820, 28725,  4109,  1233,   264,   710,  3225,  2069,\n",
      "           511, 16386,  2646,  7463,  2099, 28804,    13,  7226, 11143, 28747,\n",
      "           334, 28901, 28779,   367,   896, 20178, 29364, 28762, 12203, 28798,\n",
      "         28749,   413,  7016,  3064,  1087,  1841, 28762,   320, 15290,   725,\n",
      "           962,  1990, 28741, 13835,  4426, 28705, 28787, 28734,   420,  5244,\n",
      "         28779, 28723,  7929,  6142,   320, 15290,   725,   962,  1990, 28741,\n",
      "          9461,  4599, 28705, 28782, 28734,   420,  5244, 28779, 28723,   550,\n",
      "          2431, 28762, 14181,  3728,   384,  7408,  9420, 28723, 10415,   896,\n",
      "         28741,  6523, 28779, 18330,  9856,  1383, 29107, 16414, 10401,  1251,\n",
      "         28769, 29364, 28723,   384,  7408,   365,  4341,   330,  4202,  3728]],\n",
      "       device='cuda:0')\n",
      "Resultado: Olá, qual é a previsão do tempo para hoje?\n",
      "Assistant: CÉU PREVISÃO HOJE ENSOLARADO TEMPERATURA MEIO 70 GRAU. NOITE TEMPERATURA CAIR 50 GRAU. VENTO LEVE DIA TODO. ÁREA CHUVA POSSÍVEL AMANHÃ. DIA BOM APROVE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def limpar_gpu():\n",
    "    print(f\"Memória GPU antes de limpar: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Memória GPU depois de limpar: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "\n",
    "def gerar_texto(prompt, model_name=\"annagoncalves2/chatbot-zephyr-sft-bnb-4bit-V2\", max_length=100):\n",
    "    limpar_gpu()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Seu token de autenticação para Hugging Face Hub\n",
    "    hf_token = \"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\"\n",
    "\n",
    "    # Carregar tokenizer e modelo com token de autenticação correto\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hf_token).to(device)\n",
    "\n",
    "    # Tokenizar entrada e enviar para dispositivo\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length)\n",
    "        print(f\"Outputs: {outputs}\")\n",
    "\n",
    "    resultado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return resultado\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Olá, qual é a previsão do tempo para hoje?\"\n",
    "    print(\"Gerando texto...\")\n",
    "    resultado = gerar_texto(prompt)\n",
    "    print(\"Resultado:\", resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45cdacf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annap/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:902: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/annap/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Inferência:   0%|          | 0/15573 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 1/15573 [00:07<33:33:19,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 2/15573 [00:14<31:13:09,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 3/15573 [00:21<29:53:11,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 4/15573 [00:23<21:25:06,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 5/15573 [00:27<20:52:12,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 6/15573 [00:32<20:52:39,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 7/15573 [00:35<17:50:47,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 8/15573 [00:42<21:54:19,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 9/15573 [00:50<26:01:26,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 10/15573 [00:57<27:37:04,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 11/15573 [01:02<25:56:45,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 12/15573 [01:09<27:04:34,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 13/15573 [01:17<29:15:34,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 14/15573 [01:20<24:09:57,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 15/15573 [01:28<27:29:50,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 16/15573 [01:36<29:05:17,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 17/15573 [01:44<30:58:45,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 18/15573 [01:51<30:52:24,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 19/15573 [01:58<31:22:08,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 20/15573 [02:03<27:56:58,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 21/15573 [02:07<25:09:33,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 22/15573 [02:12<23:35:42,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 23/15573 [02:17<23:18:26,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 24/15573 [02:25<26:29:13,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 25/15573 [02:32<27:25:35,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 26/15573 [02:39<27:47:26,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 27/15573 [02:45<27:45:57,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 28/15573 [02:51<26:37:45,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 29/15573 [02:57<26:21:38,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 30/15573 [03:03<27:26:06,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 31/15573 [03:09<26:31:52,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 32/15573 [03:16<27:03:36,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 33/15573 [03:21<26:00:00,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 34/15573 [03:26<24:43:15,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 35/15573 [03:31<22:56:47,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 36/15573 [03:39<26:46:41,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 37/15573 [03:47<29:28:31,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 38/15573 [03:54<28:56:01,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 39/15573 [04:03<31:58:07,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 40/15573 [04:10<32:24:32,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 41/15573 [04:17<31:15:41,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 42/15573 [04:26<33:06:14,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 43/15573 [04:30<29:14:00,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 44/15573 [04:38<30:10:41,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 45/15573 [04:42<27:09:21,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 46/15573 [04:49<27:39:23,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 47/15573 [04:56<28:01:44,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 48/15573 [05:03<29:27:46,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 49/15573 [05:08<26:10:33,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 50/15573 [05:08<19:02:20,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 51/15573 [05:15<22:34:25,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 52/15573 [05:23<25:32:43,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 53/15573 [05:31<27:43:46,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 54/15573 [05:35<25:33:55,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 55/15573 [05:36<18:30:49,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 56/15573 [05:42<21:14:40,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 57/15573 [05:49<23:55:18,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 58/15573 [05:58<28:19:37,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 59/15573 [06:05<28:29:14,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 60/15573 [06:10<26:52:21,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 61/15573 [06:17<28:02:03,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 62/15573 [06:23<26:22:21,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 63/15573 [06:25<20:57:26,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 64/15573 [06:31<23:13:20,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 65/15573 [06:40<27:35:53,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 66/15573 [06:48<29:21:41,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 67/15573 [06:52<26:12:11,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 68/15573 [06:59<26:48:37,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 69/15573 [07:03<25:01:21,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 70/15573 [07:06<20:59:10,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 71/15573 [07:13<23:55:52,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 72/15573 [07:22<27:25:21,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 73/15573 [07:29<29:23:49,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 74/15573 [07:38<31:44:42,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 75/15573 [07:44<29:18:13,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 76/15573 [07:50<29:02:35,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   0%|          | 77/15573 [07:55<26:34:11,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 78/15573 [08:04<30:09:01,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 79/15573 [08:09<27:55:05,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 80/15573 [08:14<25:30:12,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 81/15573 [08:20<26:13:21,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 82/15573 [08:27<26:37:19,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 83/15573 [08:33<26:47:46,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 84/15573 [08:38<24:53:57,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 85/15573 [08:43<23:27:43,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 86/15573 [08:45<19:52:23,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 87/15573 [08:50<20:15:32,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 88/15573 [08:57<22:50:55,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 89/15573 [09:05<26:47:02,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 90/15573 [09:12<27:22:30,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 91/15573 [09:15<22:37:43,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 92/15573 [09:23<26:07:53,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 93/15573 [09:28<24:43:25,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 94/15573 [09:36<27:52:06,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 95/15573 [09:43<28:31:43,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 96/15573 [09:47<25:36:06,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 97/15573 [09:55<27:36:45,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 98/15573 [10:02<29:09:12,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 99/15573 [10:11<31:11:39,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 100/15573 [10:18<30:51:46,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 101/15573 [10:22<27:08:57,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 102/15573 [10:23<20:43:29,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 103/15573 [10:24<15:12:56,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 104/15573 [10:32<21:01:07,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 105/15573 [10:32<15:25:38,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 106/15573 [10:38<17:58:05,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 107/15573 [10:46<22:16:56,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 108/15573 [10:53<25:52:33,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 109/15573 [11:01<27:30:36,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 110/15573 [11:07<27:24:09,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 111/15573 [11:08<20:13:46,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 112/15573 [11:12<19:11:47,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 113/15573 [11:16<18:58:06,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 114/15573 [11:17<14:30:48,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 115/15573 [11:21<14:41:06,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 116/15573 [11:22<11:55:12,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 117/15573 [11:28<16:34:13,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 118/15573 [11:29<12:18:36,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 119/15573 [11:36<18:29:06,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 120/15573 [11:45<23:52:05,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 121/15573 [11:52<25:12:44,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 122/15573 [11:59<27:07:47,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 123/15573 [12:05<26:21:25,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 124/15573 [12:11<26:46:47,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 125/15573 [12:17<26:48:44,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 126/15573 [12:26<29:14:25,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 127/15573 [12:32<29:23:33,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 128/15573 [12:39<28:36:21,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 129/15573 [12:39<20:38:22,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 130/15573 [12:41<17:14:03,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 131/15573 [12:50<22:43:50,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 132/15573 [12:56<23:37:58,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 133/15573 [13:03<26:17:34,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 134/15573 [13:12<29:44:51,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 135/15573 [13:18<28:11:44,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 136/15573 [13:24<27:53:07,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 137/15573 [13:32<29:06:28,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 138/15573 [13:39<29:40:32,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 139/15573 [13:45<28:53:07,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 140/15573 [13:52<28:54:58,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 141/15573 [13:59<28:57:30,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 142/15573 [14:00<22:32:36,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 143/15573 [14:08<25:52:19,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 144/15573 [14:10<20:31:16,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 145/15573 [14:13<17:56:32,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 146/15573 [14:14<14:28:46,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 147/15573 [14:22<19:26:06,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 148/15573 [14:30<24:14:34,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 149/15573 [14:37<25:38:45,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 150/15573 [14:39<21:07:29,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 151/15573 [14:47<24:25:32,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 152/15573 [14:54<26:39:05,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 153/15573 [14:56<21:03:38,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 154/15573 [15:05<26:12:53,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 155/15573 [15:10<24:56:07,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 156/15573 [15:19<28:39:27,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 157/15573 [15:24<26:32:58,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 158/15573 [15:32<28:31:14,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 159/15573 [15:39<30:00:26,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 160/15573 [15:47<31:20:18,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 161/15573 [15:48<22:50:32,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 162/15573 [15:52<21:23:59,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 163/15573 [16:00<24:15:26,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 164/15573 [16:07<26:26:54,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 165/15573 [16:13<26:09:01,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 166/15573 [16:18<25:16:38,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 167/15573 [16:25<26:21:48,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 168/15573 [16:31<25:51:42,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 169/15573 [16:38<26:50:58,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 170/15573 [16:42<24:01:51,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 171/15573 [16:44<20:12:04,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 172/15573 [16:52<23:30:06,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 173/15573 [17:00<27:04:11,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 174/15573 [17:01<20:10:46,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 175/15573 [17:07<22:12:16,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 176/15573 [17:13<22:30:13,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 177/15573 [17:19<23:57:23,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 178/15573 [17:20<18:40:53,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 179/15573 [17:28<23:07:59,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 180/15573 [17:33<22:18:22,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 181/15573 [17:40<24:58:48,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 182/15573 [17:46<25:07:07,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 183/15573 [17:53<26:33:55,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 184/15573 [18:01<28:54:54,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 185/15573 [18:09<30:38:57,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 186/15573 [18:14<27:26:58,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 187/15573 [18:17<22:28:33,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 188/15573 [18:24<25:13:56,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 189/15573 [18:25<19:11:14,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 190/15573 [18:34<24:01:56,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 191/15573 [18:40<25:18:50,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 192/15573 [18:42<20:30:41,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 193/15573 [18:46<18:53:39,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|          | 194/15573 [18:51<20:14:55,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 195/15573 [18:58<22:39:08,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 196/15573 [19:04<22:52:48,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 197/15573 [19:11<25:28:43,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 198/15573 [19:14<22:10:18,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 199/15573 [19:21<24:27:43,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 200/15573 [19:23<18:56:09,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 201/15573 [19:28<20:33:48,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 202/15573 [19:34<21:15:18,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 203/15573 [19:40<22:28:58,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 204/15573 [19:46<24:23:14,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 205/15573 [19:49<20:55:23,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 206/15573 [19:53<19:51:09,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 207/15573 [19:54<14:42:10,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 208/15573 [20:01<18:45:15,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 209/15573 [20:09<23:35:55,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 210/15573 [20:14<22:47:27,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 211/15573 [20:19<22:07:03,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 212/15573 [20:20<17:15:23,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 213/15573 [20:21<12:53:01,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 214/15573 [20:29<20:17:04,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 215/15573 [20:36<22:11:35,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 216/15573 [20:43<25:27:02,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 217/15573 [20:52<28:24:18,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 218/15573 [20:59<29:12:51,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 219/15573 [21:05<27:46:12,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 220/15573 [21:12<28:47:28,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 221/15573 [21:13<20:45:46,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 222/15573 [21:19<22:35:05,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 223/15573 [21:26<25:06:27,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 224/15573 [21:33<26:42:46,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 225/15573 [21:40<27:50:41,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 226/15573 [21:44<23:37:18,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 227/15573 [21:44<17:08:39,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 228/15573 [21:45<12:42:10,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 229/15573 [21:53<19:39:07,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 230/15573 [22:01<23:49:43,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 231/15573 [22:08<26:19:49,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 232/15573 [22:17<28:53:39,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   1%|▏         | 233/15573 [22:22<27:16:24,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   2%|▏         | 234/15573 [22:30<29:04:41,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   2%|▏         | 235/15573 [22:31<22:09:53,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   2%|▏         | 236/15573 [22:38<23:39:42,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   2%|▏         | 237/15573 [22:45<25:57:45,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   2%|▏         | 238/15573 [22:51<25:06:51,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   2%|▏         | 239/15573 [22:52<18:53:54,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   2%|▏         | 240/15573 [22:58<21:16:30,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   2%|▏         | 241/15573 [23:03<21:19:38,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   2%|▏         | 242/15573 [23:04<16:38:24,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   2%|▏         | 243/15573 [23:08<16:33:31,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferência:   2%|▏         | 244/15573 [23:17<24:23:16,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória GPU antes de limpar: 12799.64 MB\n",
      "Memória GPU depois de limpar: 12799.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 248, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m row[prompt_col]\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m#print(f\"Gerando saída para linha {i}: {prompt}\")\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mgerar_texto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Adicionar a nova coluna ao dataframe\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mgerar_texto\u001b[0;34m(prompt, tokenizer, model, device, max_length)\u001b[0m\n\u001b[1;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m resultado \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resultado\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2453\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_logits_to_keep() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   2451\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[1;32m   2456\u001b[0m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[1;32m   2457\u001b[0m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[1;32m   2458\u001b[0m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[1;32m   2459\u001b[0m max_cache_length \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1617\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1616\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1617\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1618\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1619\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1621\u001b[0m     )\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1624\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1625\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1626\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1627\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 248, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "\n",
    "ds_org_eng = \"/home/annap/Documents/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_english.csv\"\n",
    "ds_output_dir = \"/home/annap/Documents/chatbot_copy/INFERENCE/DATASETS/UNSLOTH/\"\n",
    "\n",
    "def limpar_gpu():\n",
    "    print(f\"Memória GPU antes de limpar: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Memória GPU depois de limpar: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "\n",
    "def gerar_texto(prompt, tokenizer, model, device, max_length=200):\n",
    "    limpar_gpu()\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "    resultado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return resultado\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hf_token = \"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\"\n",
    "    model_name = \"annagoncalves2/chatbot-zephyr-sft-bnb-4bit-V2\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Carregar tokenizer e modelo uma vez só\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hf_token).to(device)\n",
    "\n",
    "    # Ler o CSV\n",
    "    df = pd.read_csv(ds_org_eng)\n",
    "\n",
    "    # Supondo que a coluna que contém o prompt seja chamada \"prompt\"\n",
    "    # Se o nome da coluna for outro, substitua aqui:\n",
    "    prompt_col = \"prompt\"  # ajuste conforme seu CSV\n",
    "\n",
    "    outputs = []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Inferência\"):\n",
    "        prompt = row[prompt_col]\n",
    "        #print(f\"Gerando saída para linha {i}: {prompt}\")\n",
    "        out = gerar_texto(prompt, tokenizer, model, device)\n",
    "        outputs.append(out)\n",
    "\n",
    "    # Adicionar a nova coluna ao dataframe\n",
    "    df[\"output\"] = outputs\n",
    "\n",
    "    # Salvar o dataframe novo\n",
    "    filename = os.path.basename(ds_org_eng).replace(\".csv\", \"_inference.csv\")\n",
    "    output_path = os.path.join(ds_output_dir, filename)\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Inferência salva em: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c9508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annap/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:902: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/annap/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/annap/Documents/chatbot_copy/DATASETS/MMLU/0_mmlu_glosa_pt.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, use_auth_token\u001b[38;5;241m=\u001b[39mhf_token)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Ler o CSV\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_org_eng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Supondo que a coluna que contém o prompt seja chamada \"prompt\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Se o nome da coluna for outro, substitua aqui:\u001b[39;00m\n\u001b[1;32m     39\u001b[0m prompt_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# ajuste conforme seu CSV\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/annap/Documents/chatbot_copy/DATASETS/MMLU/0_mmlu_glosa_pt.csv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "\n",
    "ds_org_eng = \"/home/annap/Documents/chatbot_copy/DATASETS/MMLU/0_mmlu_prompt_glosa_pt.csv\"\n",
    "ds_output_dir = \"/home/annap/Documents/chatbot_copy/INFERENCE/DATASETS/UNSLOTH/\"\n",
    "\n",
    "def limpar_gpu():\n",
    "    print(f\"Memória GPU antes de limpar: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Memória GPU depois de limpar: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "\n",
    "def gerar_texto(prompt, tokenizer, model, device, max_length=200):\n",
    "    limpar_gpu()\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "    resultado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return resultado\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hf_token = \"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\"\n",
    "    model_name = \"annagoncalves2/chatbot-zephyr-sft-bnb-4bit-V2\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Carregar tokenizer e modelo uma vez só\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hf_token).to(device)\n",
    "\n",
    "    # Ler o CSV\n",
    "    df = pd.read_csv(ds_org_eng)\n",
    "\n",
    "    # Supondo que a coluna que contém o prompt seja chamada \"prompt\"\n",
    "    # Se o nome da coluna for outro, substitua aqui:\n",
    "    prompt_col = \"prompt\"  # ajuste conforme seu CSV\n",
    "\n",
    "    outputs = []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Inferência\"):\n",
    "        prompt = row[prompt_col]\n",
    "        #print(f\"Gerando saída para linha {i}: {prompt}\")\n",
    "        out = gerar_texto(prompt, tokenizer, model, device)\n",
    "        outputs.append(out)\n",
    "\n",
    "    # Adicionar a nova coluna ao dataframe\n",
    "    df[\"output\"] = outputs\n",
    "\n",
    "    # Salvar o dataframe novo\n",
    "    filename = os.path.basename(ds_org_eng).replace(\".csv\", \"_glosa_inference.csv\")\n",
    "    output_path = os.path.join(ds_output_dir, filename)\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Inferência salva em: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc96cc",
   "metadata": {},
   "source": [
    "# Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2478f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando modelo annagoncalves2/chatbot-gemma-3-12b-it-bnb-4bit-V2 para ./models/chatbot-gemma-3-12b-it-bnb-4bit-V2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]\n",
      "Loading adapter weights from annagoncalves2/chatbot-gemma-3-12b-it-bnb-4bit-V2 led to unexpected keys not found in the model: language_model.model.layers.0.mlp.down_proj.lora_A.default.weight, language_model.model.layers.0.mlp.down_proj.lora_B.default.weight, language_model.model.layers.0.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.0.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.0.mlp.up_proj.lora_A.default.weight, language_model.model.layers.0.mlp.up_proj.lora_B.default.weight, language_model.model.layers.0.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.0.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.0.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.0.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.0.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.0.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.0.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.0.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.1.mlp.down_proj.lora_A.default.weight, language_model.model.layers.1.mlp.down_proj.lora_B.default.weight, language_model.model.layers.1.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.1.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.1.mlp.up_proj.lora_A.default.weight, language_model.model.layers.1.mlp.up_proj.lora_B.default.weight, language_model.model.layers.1.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.1.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.1.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.1.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.1.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.1.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.1.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.1.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.10.mlp.down_proj.lora_A.default.weight, language_model.model.layers.10.mlp.down_proj.lora_B.default.weight, language_model.model.layers.10.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.10.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.10.mlp.up_proj.lora_A.default.weight, language_model.model.layers.10.mlp.up_proj.lora_B.default.weight, language_model.model.layers.10.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.10.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.10.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.10.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.10.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.10.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.10.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.10.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.11.mlp.down_proj.lora_A.default.weight, language_model.model.layers.11.mlp.down_proj.lora_B.default.weight, language_model.model.layers.11.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.11.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.11.mlp.up_proj.lora_A.default.weight, language_model.model.layers.11.mlp.up_proj.lora_B.default.weight, language_model.model.layers.11.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.11.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.11.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.11.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.11.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.11.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.11.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.11.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.12.mlp.down_proj.lora_A.default.weight, language_model.model.layers.12.mlp.down_proj.lora_B.default.weight, language_model.model.layers.12.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.12.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.12.mlp.up_proj.lora_A.default.weight, language_model.model.layers.12.mlp.up_proj.lora_B.default.weight, language_model.model.layers.12.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.12.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.12.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.12.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.12.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.12.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.12.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.12.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.13.mlp.down_proj.lora_A.default.weight, language_model.model.layers.13.mlp.down_proj.lora_B.default.weight, language_model.model.layers.13.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.13.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.13.mlp.up_proj.lora_A.default.weight, language_model.model.layers.13.mlp.up_proj.lora_B.default.weight, language_model.model.layers.13.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.13.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.13.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.13.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.13.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.13.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.13.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.13.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.14.mlp.down_proj.lora_A.default.weight, language_model.model.layers.14.mlp.down_proj.lora_B.default.weight, language_model.model.layers.14.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.14.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.14.mlp.up_proj.lora_A.default.weight, language_model.model.layers.14.mlp.up_proj.lora_B.default.weight, language_model.model.layers.14.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.14.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.14.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.14.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.14.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.14.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.14.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.14.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.15.mlp.down_proj.lora_A.default.weight, language_model.model.layers.15.mlp.down_proj.lora_B.default.weight, language_model.model.layers.15.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.15.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.15.mlp.up_proj.lora_A.default.weight, language_model.model.layers.15.mlp.up_proj.lora_B.default.weight, language_model.model.layers.15.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.15.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.15.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.15.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.15.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.15.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.15.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.15.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.16.mlp.down_proj.lora_A.default.weight, language_model.model.layers.16.mlp.down_proj.lora_B.default.weight, language_model.model.layers.16.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.16.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.16.mlp.up_proj.lora_A.default.weight, language_model.model.layers.16.mlp.up_proj.lora_B.default.weight, language_model.model.layers.16.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.16.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.16.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.16.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.16.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.16.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.16.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.16.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.17.mlp.down_proj.lora_A.default.weight, language_model.model.layers.17.mlp.down_proj.lora_B.default.weight, language_model.model.layers.17.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.17.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.17.mlp.up_proj.lora_A.default.weight, language_model.model.layers.17.mlp.up_proj.lora_B.default.weight, language_model.model.layers.17.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.17.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.17.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.17.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.17.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.17.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.17.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.17.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.18.mlp.down_proj.lora_A.default.weight, language_model.model.layers.18.mlp.down_proj.lora_B.default.weight, language_model.model.layers.18.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.18.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.18.mlp.up_proj.lora_A.default.weight, language_model.model.layers.18.mlp.up_proj.lora_B.default.weight, language_model.model.layers.18.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.18.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.18.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.18.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.18.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.18.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.18.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.18.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.19.mlp.down_proj.lora_A.default.weight, language_model.model.layers.19.mlp.down_proj.lora_B.default.weight, language_model.model.layers.19.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.19.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.19.mlp.up_proj.lora_A.default.weight, language_model.model.layers.19.mlp.up_proj.lora_B.default.weight, language_model.model.layers.19.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.19.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.19.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.19.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.19.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.19.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.19.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.19.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.2.mlp.down_proj.lora_A.default.weight, language_model.model.layers.2.mlp.down_proj.lora_B.default.weight, language_model.model.layers.2.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.2.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.2.mlp.up_proj.lora_A.default.weight, language_model.model.layers.2.mlp.up_proj.lora_B.default.weight, language_model.model.layers.2.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.2.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.2.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.2.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.2.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.2.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.2.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.2.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.20.mlp.down_proj.lora_A.default.weight, language_model.model.layers.20.mlp.down_proj.lora_B.default.weight, language_model.model.layers.20.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.20.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.20.mlp.up_proj.lora_A.default.weight, language_model.model.layers.20.mlp.up_proj.lora_B.default.weight, language_model.model.layers.20.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.20.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.20.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.20.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.20.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.20.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.20.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.20.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.21.mlp.down_proj.lora_A.default.weight, language_model.model.layers.21.mlp.down_proj.lora_B.default.weight, language_model.model.layers.21.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.21.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.21.mlp.up_proj.lora_A.default.weight, language_model.model.layers.21.mlp.up_proj.lora_B.default.weight, language_model.model.layers.21.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.21.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.21.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.21.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.21.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.21.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.21.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.21.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.22.mlp.down_proj.lora_A.default.weight, language_model.model.layers.22.mlp.down_proj.lora_B.default.weight, language_model.model.layers.22.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.22.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.22.mlp.up_proj.lora_A.default.weight, language_model.model.layers.22.mlp.up_proj.lora_B.default.weight, language_model.model.layers.22.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.22.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.22.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.22.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.22.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.22.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.22.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.22.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.23.mlp.down_proj.lora_A.default.weight, language_model.model.layers.23.mlp.down_proj.lora_B.default.weight, language_model.model.layers.23.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.23.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.23.mlp.up_proj.lora_A.default.weight, language_model.model.layers.23.mlp.up_proj.lora_B.default.weight, language_model.model.layers.23.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.23.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.23.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.23.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.23.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.23.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.23.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.23.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.24.mlp.down_proj.lora_A.default.weight, language_model.model.layers.24.mlp.down_proj.lora_B.default.weight, language_model.model.layers.24.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.24.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.24.mlp.up_proj.lora_A.default.weight, language_model.model.layers.24.mlp.up_proj.lora_B.default.weight, language_model.model.layers.24.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.24.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.24.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.24.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.24.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.24.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.24.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.24.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.25.mlp.down_proj.lora_A.default.weight, language_model.model.layers.25.mlp.down_proj.lora_B.default.weight, language_model.model.layers.25.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.25.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.25.mlp.up_proj.lora_A.default.weight, language_model.model.layers.25.mlp.up_proj.lora_B.default.weight, language_model.model.layers.25.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.25.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.25.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.25.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.25.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.25.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.25.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.25.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.26.mlp.down_proj.lora_A.default.weight, language_model.model.layers.26.mlp.down_proj.lora_B.default.weight, language_model.model.layers.26.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.26.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.26.mlp.up_proj.lora_A.default.weight, language_model.model.layers.26.mlp.up_proj.lora_B.default.weight, language_model.model.layers.26.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.26.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.26.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.26.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.26.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.26.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.26.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.26.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.27.mlp.down_proj.lora_A.default.weight, language_model.model.layers.27.mlp.down_proj.lora_B.default.weight, language_model.model.layers.27.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.27.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.27.mlp.up_proj.lora_A.default.weight, language_model.model.layers.27.mlp.up_proj.lora_B.default.weight, language_model.model.layers.27.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.27.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.27.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.27.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.27.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.27.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.27.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.27.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.28.mlp.down_proj.lora_A.default.weight, language_model.model.layers.28.mlp.down_proj.lora_B.default.weight, language_model.model.layers.28.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.28.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.28.mlp.up_proj.lora_A.default.weight, language_model.model.layers.28.mlp.up_proj.lora_B.default.weight, language_model.model.layers.28.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.28.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.28.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.28.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.28.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.28.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.28.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.28.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.29.mlp.down_proj.lora_A.default.weight, language_model.model.layers.29.mlp.down_proj.lora_B.default.weight, language_model.model.layers.29.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.29.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.29.mlp.up_proj.lora_A.default.weight, language_model.model.layers.29.mlp.up_proj.lora_B.default.weight, language_model.model.layers.29.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.29.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.29.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.29.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.29.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.29.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.29.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.29.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.3.mlp.down_proj.lora_A.default.weight, language_model.model.layers.3.mlp.down_proj.lora_B.default.weight, language_model.model.layers.3.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.3.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.3.mlp.up_proj.lora_A.default.weight, language_model.model.layers.3.mlp.up_proj.lora_B.default.weight, language_model.model.layers.3.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.3.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.3.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.3.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.3.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.3.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.3.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.3.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.30.mlp.down_proj.lora_A.default.weight, language_model.model.layers.30.mlp.down_proj.lora_B.default.weight, language_model.model.layers.30.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.30.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.30.mlp.up_proj.lora_A.default.weight, language_model.model.layers.30.mlp.up_proj.lora_B.default.weight, language_model.model.layers.30.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.30.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.30.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.30.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.30.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.30.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.30.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.30.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.31.mlp.down_proj.lora_A.default.weight, language_model.model.layers.31.mlp.down_proj.lora_B.default.weight, language_model.model.layers.31.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.31.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.31.mlp.up_proj.lora_A.default.weight, language_model.model.layers.31.mlp.up_proj.lora_B.default.weight, language_model.model.layers.31.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.31.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.31.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.31.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.31.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.31.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.31.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.31.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.32.mlp.down_proj.lora_A.default.weight, language_model.model.layers.32.mlp.down_proj.lora_B.default.weight, language_model.model.layers.32.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.32.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.32.mlp.up_proj.lora_A.default.weight, language_model.model.layers.32.mlp.up_proj.lora_B.default.weight, language_model.model.layers.32.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.32.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.32.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.32.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.32.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.32.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.32.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.32.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.33.mlp.down_proj.lora_A.default.weight, language_model.model.layers.33.mlp.down_proj.lora_B.default.weight, language_model.model.layers.33.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.33.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.33.mlp.up_proj.lora_A.default.weight, language_model.model.layers.33.mlp.up_proj.lora_B.default.weight, language_model.model.layers.33.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.33.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.33.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.33.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.33.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.33.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.33.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.33.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.34.mlp.down_proj.lora_A.default.weight, language_model.model.layers.34.mlp.down_proj.lora_B.default.weight, language_model.model.layers.34.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.34.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.34.mlp.up_proj.lora_A.default.weight, language_model.model.layers.34.mlp.up_proj.lora_B.default.weight, language_model.model.layers.34.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.34.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.34.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.34.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.34.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.34.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.34.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.34.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.35.mlp.down_proj.lora_A.default.weight, language_model.model.layers.35.mlp.down_proj.lora_B.default.weight, language_model.model.layers.35.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.35.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.35.mlp.up_proj.lora_A.default.weight, language_model.model.layers.35.mlp.up_proj.lora_B.default.weight, language_model.model.layers.35.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.35.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.35.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.35.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.35.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.35.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.35.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.35.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.36.mlp.down_proj.lora_A.default.weight, language_model.model.layers.36.mlp.down_proj.lora_B.default.weight, language_model.model.layers.36.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.36.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.36.mlp.up_proj.lora_A.default.weight, language_model.model.layers.36.mlp.up_proj.lora_B.default.weight, language_model.model.layers.36.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.36.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.36.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.36.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.36.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.36.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.36.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.36.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.37.mlp.down_proj.lora_A.default.weight, language_model.model.layers.37.mlp.down_proj.lora_B.default.weight, language_model.model.layers.37.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.37.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.37.mlp.up_proj.lora_A.default.weight, language_model.model.layers.37.mlp.up_proj.lora_B.default.weight, language_model.model.layers.37.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.37.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.37.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.37.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.37.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.37.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.37.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.37.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.38.mlp.down_proj.lora_A.default.weight, language_model.model.layers.38.mlp.down_proj.lora_B.default.weight, language_model.model.layers.38.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.38.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.38.mlp.up_proj.lora_A.default.weight, language_model.model.layers.38.mlp.up_proj.lora_B.default.weight, language_model.model.layers.38.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.38.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.38.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.38.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.38.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.38.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.38.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.38.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.39.mlp.down_proj.lora_A.default.weight, language_model.model.layers.39.mlp.down_proj.lora_B.default.weight, language_model.model.layers.39.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.39.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.39.mlp.up_proj.lora_A.default.weight, language_model.model.layers.39.mlp.up_proj.lora_B.default.weight, language_model.model.layers.39.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.39.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.39.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.39.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.39.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.39.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.39.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.39.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.4.mlp.down_proj.lora_A.default.weight, language_model.model.layers.4.mlp.down_proj.lora_B.default.weight, language_model.model.layers.4.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.4.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.4.mlp.up_proj.lora_A.default.weight, language_model.model.layers.4.mlp.up_proj.lora_B.default.weight, language_model.model.layers.4.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.4.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.4.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.4.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.4.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.4.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.4.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.4.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.40.mlp.down_proj.lora_A.default.weight, language_model.model.layers.40.mlp.down_proj.lora_B.default.weight, language_model.model.layers.40.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.40.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.40.mlp.up_proj.lora_A.default.weight, language_model.model.layers.40.mlp.up_proj.lora_B.default.weight, language_model.model.layers.40.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.40.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.40.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.40.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.40.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.40.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.40.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.40.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.41.mlp.down_proj.lora_A.default.weight, language_model.model.layers.41.mlp.down_proj.lora_B.default.weight, language_model.model.layers.41.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.41.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.41.mlp.up_proj.lora_A.default.weight, language_model.model.layers.41.mlp.up_proj.lora_B.default.weight, language_model.model.layers.41.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.41.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.41.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.41.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.41.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.41.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.41.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.41.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.42.mlp.down_proj.lora_A.default.weight, language_model.model.layers.42.mlp.down_proj.lora_B.default.weight, language_model.model.layers.42.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.42.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.42.mlp.up_proj.lora_A.default.weight, language_model.model.layers.42.mlp.up_proj.lora_B.default.weight, language_model.model.layers.42.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.42.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.42.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.42.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.42.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.42.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.42.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.42.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.43.mlp.down_proj.lora_A.default.weight, language_model.model.layers.43.mlp.down_proj.lora_B.default.weight, language_model.model.layers.43.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.43.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.43.mlp.up_proj.lora_A.default.weight, language_model.model.layers.43.mlp.up_proj.lora_B.default.weight, language_model.model.layers.43.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.43.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.43.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.43.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.43.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.43.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.43.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.43.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.44.mlp.down_proj.lora_A.default.weight, language_model.model.layers.44.mlp.down_proj.lora_B.default.weight, language_model.model.layers.44.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.44.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.44.mlp.up_proj.lora_A.default.weight, language_model.model.layers.44.mlp.up_proj.lora_B.default.weight, language_model.model.layers.44.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.44.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.44.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.44.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.44.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.44.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.44.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.44.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.45.mlp.down_proj.lora_A.default.weight, language_model.model.layers.45.mlp.down_proj.lora_B.default.weight, language_model.model.layers.45.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.45.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.45.mlp.up_proj.lora_A.default.weight, language_model.model.layers.45.mlp.up_proj.lora_B.default.weight, language_model.model.layers.45.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.45.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.45.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.45.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.45.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.45.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.45.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.45.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.46.mlp.down_proj.lora_A.default.weight, language_model.model.layers.46.mlp.down_proj.lora_B.default.weight, language_model.model.layers.46.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.46.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.46.mlp.up_proj.lora_A.default.weight, language_model.model.layers.46.mlp.up_proj.lora_B.default.weight, language_model.model.layers.46.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.46.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.46.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.46.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.46.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.46.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.46.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.46.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.47.mlp.down_proj.lora_A.default.weight, language_model.model.layers.47.mlp.down_proj.lora_B.default.weight, language_model.model.layers.47.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.47.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.47.mlp.up_proj.lora_A.default.weight, language_model.model.layers.47.mlp.up_proj.lora_B.default.weight, language_model.model.layers.47.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.47.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.47.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.47.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.47.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.47.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.47.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.47.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.5.mlp.down_proj.lora_A.default.weight, language_model.model.layers.5.mlp.down_proj.lora_B.default.weight, language_model.model.layers.5.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.5.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.5.mlp.up_proj.lora_A.default.weight, language_model.model.layers.5.mlp.up_proj.lora_B.default.weight, language_model.model.layers.5.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.5.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.5.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.5.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.5.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.5.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.5.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.5.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.6.mlp.down_proj.lora_A.default.weight, language_model.model.layers.6.mlp.down_proj.lora_B.default.weight, language_model.model.layers.6.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.6.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.6.mlp.up_proj.lora_A.default.weight, language_model.model.layers.6.mlp.up_proj.lora_B.default.weight, language_model.model.layers.6.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.6.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.6.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.6.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.6.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.6.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.6.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.6.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.7.mlp.down_proj.lora_A.default.weight, language_model.model.layers.7.mlp.down_proj.lora_B.default.weight, language_model.model.layers.7.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.7.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.7.mlp.up_proj.lora_A.default.weight, language_model.model.layers.7.mlp.up_proj.lora_B.default.weight, language_model.model.layers.7.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.7.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.7.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.7.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.7.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.7.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.7.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.7.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.8.mlp.down_proj.lora_A.default.weight, language_model.model.layers.8.mlp.down_proj.lora_B.default.weight, language_model.model.layers.8.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.8.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.8.mlp.up_proj.lora_A.default.weight, language_model.model.layers.8.mlp.up_proj.lora_B.default.weight, language_model.model.layers.8.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.8.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.8.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.8.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.8.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.8.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.8.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.8.self_attn.v_proj.lora_B.default.weight, language_model.model.layers.9.mlp.down_proj.lora_A.default.weight, language_model.model.layers.9.mlp.down_proj.lora_B.default.weight, language_model.model.layers.9.mlp.gate_proj.lora_A.default.weight, language_model.model.layers.9.mlp.gate_proj.lora_B.default.weight, language_model.model.layers.9.mlp.up_proj.lora_A.default.weight, language_model.model.layers.9.mlp.up_proj.lora_B.default.weight, language_model.model.layers.9.self_attn.k_proj.lora_A.default.weight, language_model.model.layers.9.self_attn.k_proj.lora_B.default.weight, language_model.model.layers.9.self_attn.o_proj.lora_A.default.weight, language_model.model.layers.9.self_attn.o_proj.lora_B.default.weight, language_model.model.layers.9.self_attn.q_proj.lora_A.default.weight, language_model.model.layers.9.self_attn.q_proj.lora_B.default.weight, language_model.model.layers.9.self_attn.v_proj.lora_A.default.weight, language_model.model.layers.9.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight, vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight, vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight. Loading adapter weights from annagoncalves2/chatbot-gemma-3-12b-it-bnb-4bit-V2 led to missing keys in the model: model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.lora_B.default.weight, model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.lora_A.default.weight, model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, model.language_model.layers.27.mlp.down_proj.lora_B.default.weight, model.language_model.layers.28.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.28.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.28.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.28.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.28.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.28.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.28.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.28.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.28.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.28.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.28.mlp.up_proj.lora_A.default.weight, model.language_model.layers.28.mlp.up_proj.lora_B.default.weight, model.language_model.layers.28.mlp.down_proj.lora_A.default.weight, model.language_model.layers.28.mlp.down_proj.lora_B.default.weight, model.language_model.layers.29.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.29.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.29.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.29.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.29.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.29.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.29.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.29.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.29.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.29.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.29.mlp.up_proj.lora_A.default.weight, model.language_model.layers.29.mlp.up_proj.lora_B.default.weight, model.language_model.layers.29.mlp.down_proj.lora_A.default.weight, model.language_model.layers.29.mlp.down_proj.lora_B.default.weight, model.language_model.layers.30.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.30.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.30.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.30.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.30.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.30.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.30.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.30.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.30.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.30.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.30.mlp.up_proj.lora_A.default.weight, model.language_model.layers.30.mlp.up_proj.lora_B.default.weight, model.language_model.layers.30.mlp.down_proj.lora_A.default.weight, model.language_model.layers.30.mlp.down_proj.lora_B.default.weight, model.language_model.layers.31.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.31.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.31.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.31.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.31.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.31.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.31.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.31.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.31.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.31.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.31.mlp.up_proj.lora_A.default.weight, model.language_model.layers.31.mlp.up_proj.lora_B.default.weight, model.language_model.layers.31.mlp.down_proj.lora_A.default.weight, model.language_model.layers.31.mlp.down_proj.lora_B.default.weight, model.language_model.layers.32.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.32.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.32.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.32.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.32.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.32.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.32.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.32.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.32.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.32.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.32.mlp.up_proj.lora_A.default.weight, model.language_model.layers.32.mlp.up_proj.lora_B.default.weight, model.language_model.layers.32.mlp.down_proj.lora_A.default.weight, model.language_model.layers.32.mlp.down_proj.lora_B.default.weight, model.language_model.layers.33.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.33.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.33.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.33.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.33.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.33.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.33.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.33.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.33.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.33.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.33.mlp.up_proj.lora_A.default.weight, model.language_model.layers.33.mlp.up_proj.lora_B.default.weight, model.language_model.layers.33.mlp.down_proj.lora_A.default.weight, model.language_model.layers.33.mlp.down_proj.lora_B.default.weight, model.language_model.layers.34.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.34.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.34.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.34.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.34.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.34.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.34.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.34.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.34.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.34.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.34.mlp.up_proj.lora_A.default.weight, model.language_model.layers.34.mlp.up_proj.lora_B.default.weight, model.language_model.layers.34.mlp.down_proj.lora_A.default.weight, model.language_model.layers.34.mlp.down_proj.lora_B.default.weight, model.language_model.layers.35.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.35.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.35.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.35.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.35.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.35.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.35.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.35.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.35.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.35.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.35.mlp.up_proj.lora_A.default.weight, model.language_model.layers.35.mlp.up_proj.lora_B.default.weight, model.language_model.layers.35.mlp.down_proj.lora_A.default.weight, model.language_model.layers.35.mlp.down_proj.lora_B.default.weight, model.language_model.layers.36.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.36.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.36.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.36.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.36.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.36.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.36.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.36.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.36.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.36.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.36.mlp.up_proj.lora_A.default.weight, model.language_model.layers.36.mlp.up_proj.lora_B.default.weight, model.language_model.layers.36.mlp.down_proj.lora_A.default.weight, model.language_model.layers.36.mlp.down_proj.lora_B.default.weight, model.language_model.layers.37.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.37.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.37.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.37.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.37.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.37.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.37.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.37.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.37.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.37.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.37.mlp.up_proj.lora_A.default.weight, model.language_model.layers.37.mlp.up_proj.lora_B.default.weight, model.language_model.layers.37.mlp.down_proj.lora_A.default.weight, model.language_model.layers.37.mlp.down_proj.lora_B.default.weight, model.language_model.layers.38.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.38.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.38.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.38.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.38.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.38.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.38.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.38.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.38.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.38.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.38.mlp.up_proj.lora_A.default.weight, model.language_model.layers.38.mlp.up_proj.lora_B.default.weight, model.language_model.layers.38.mlp.down_proj.lora_A.default.weight, model.language_model.layers.38.mlp.down_proj.lora_B.default.weight, model.language_model.layers.39.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.39.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.39.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.39.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.39.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.39.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.39.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.39.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.39.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.39.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.39.mlp.up_proj.lora_A.default.weight, model.language_model.layers.39.mlp.up_proj.lora_B.default.weight, model.language_model.layers.39.mlp.down_proj.lora_A.default.weight, model.language_model.layers.39.mlp.down_proj.lora_B.default.weight, model.language_model.layers.40.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.40.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.40.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.40.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.40.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.40.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.40.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.40.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.40.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.40.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.40.mlp.up_proj.lora_A.default.weight, model.language_model.layers.40.mlp.up_proj.lora_B.default.weight, model.language_model.layers.40.mlp.down_proj.lora_A.default.weight, model.language_model.layers.40.mlp.down_proj.lora_B.default.weight, model.language_model.layers.41.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.41.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.41.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.41.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.41.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.41.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.41.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.41.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.41.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.41.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.41.mlp.up_proj.lora_A.default.weight, model.language_model.layers.41.mlp.up_proj.lora_B.default.weight, model.language_model.layers.41.mlp.down_proj.lora_A.default.weight, model.language_model.layers.41.mlp.down_proj.lora_B.default.weight, model.language_model.layers.42.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.42.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.42.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.42.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.42.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.42.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.42.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.42.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.42.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.42.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.42.mlp.up_proj.lora_A.default.weight, model.language_model.layers.42.mlp.up_proj.lora_B.default.weight, model.language_model.layers.42.mlp.down_proj.lora_A.default.weight, model.language_model.layers.42.mlp.down_proj.lora_B.default.weight, model.language_model.layers.43.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.43.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.43.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.43.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.43.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.43.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.43.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.43.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.43.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.43.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.43.mlp.up_proj.lora_A.default.weight, model.language_model.layers.43.mlp.up_proj.lora_B.default.weight, model.language_model.layers.43.mlp.down_proj.lora_A.default.weight, model.language_model.layers.43.mlp.down_proj.lora_B.default.weight, model.language_model.layers.44.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.44.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.44.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.44.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.44.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.44.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.44.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.44.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.44.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.44.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.44.mlp.up_proj.lora_A.default.weight, model.language_model.layers.44.mlp.up_proj.lora_B.default.weight, model.language_model.layers.44.mlp.down_proj.lora_A.default.weight, model.language_model.layers.44.mlp.down_proj.lora_B.default.weight, model.language_model.layers.45.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.45.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.45.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.45.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.45.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.45.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.45.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.45.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.45.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.45.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.45.mlp.up_proj.lora_A.default.weight, model.language_model.layers.45.mlp.up_proj.lora_B.default.weight, model.language_model.layers.45.mlp.down_proj.lora_A.default.weight, model.language_model.layers.45.mlp.down_proj.lora_B.default.weight, model.language_model.layers.46.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.46.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.46.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.46.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.46.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.46.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.46.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.46.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.46.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.46.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.46.mlp.up_proj.lora_A.default.weight, model.language_model.layers.46.mlp.up_proj.lora_B.default.weight, model.language_model.layers.46.mlp.down_proj.lora_A.default.weight, model.language_model.layers.46.mlp.down_proj.lora_B.default.weight, model.language_model.layers.47.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.47.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.47.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.47.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.47.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.47.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.47.self_attn.o_proj.lora_A.default.weight, model.language_model.layers.47.self_attn.o_proj.lora_B.default.weight, model.language_model.layers.47.mlp.gate_proj.lora_A.default.weight, model.language_model.layers.47.mlp.gate_proj.lora_B.default.weight, model.language_model.layers.47.mlp.up_proj.lora_A.default.weight, model.language_model.layers.47.mlp.up_proj.lora_B.default.weight, model.language_model.layers.47.mlp.down_proj.lora_A.default.weight, model.language_model.layers.47.mlp.down_proj.lora_B.default.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo annagoncalves2/chatbot-gemma-3-12b-it-bnb-4bit-V2 salvo em ./models/chatbot-gemma-3-12b-it-bnb-4bit-V2\n",
      "Modelo já existe localmente em ./models/chatbot-Llama-3.1-8B-unsloth-bnb-4bit-V2, pulando download.\n",
      "Baixando modelo annagoncalves2/chatbot-Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V2 para ./models/chatbot-Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V2 ...\n",
      "Modelo annagoncalves2/chatbot-Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V2 salvo em ./models/chatbot-Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V2\n",
      "Baixando modelo annagoncalves2/chatbot-phi-4-unsloth-bnb-4bit-V2 para ./models/chatbot-phi-4-unsloth-bnb-4bit-V2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo annagoncalves2/chatbot-phi-4-unsloth-bnb-4bit-V2 salvo em ./models/chatbot-phi-4-unsloth-bnb-4bit-V2\n",
      "Baixando modelo annagoncalves2/chatbot-Qwen2.5-7B-Instruct-bnb-4bit-V2 para ./models/chatbot-Qwen2.5-7B-Instruct-bnb-4bit-V2 ...\n",
      "Modelo annagoncalves2/chatbot-Qwen2.5-7B-Instruct-bnb-4bit-V2 salvo em ./models/chatbot-Qwen2.5-7B-Instruct-bnb-4bit-V2\n",
      "Baixando modelo annagoncalves2/chatbot-Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V2 para ./models/chatbot-Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao baixar annagoncalves2/chatbot-Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V2: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 23.68 GiB of which 736.44 MiB is free. Process 1625 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 22.60 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 241.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Baixando modelo annagoncalves2/chatbot-zephyr-sft-bnb-4bit-V2 para ./models/chatbot-zephyr-sft-bnb-4bit-V2 ...\n",
      "Erro ao baixar annagoncalves2/chatbot-zephyr-sft-bnb-4bit-V2: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 12.44 MiB is free. Process 1625 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 23.30 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 27.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Baixando modelo annagoncalves2/chatbot-DeepSeek-R1-Distill-Llama-8B-V2 para ./models/chatbot-DeepSeek-R1-Distill-Llama-8B-V2 ...\n",
      "Erro ao baixar annagoncalves2/chatbot-DeepSeek-R1-Distill-Llama-8B-V2: Unrecognized model in annagoncalves2/chatbot-DeepSeek-R1-Distill-Llama-8B-V2. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mlcd, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "token = \"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\"\n",
    "\n",
    "model_names = [\n",
    "   \"annagoncalves2/chatbot-gemma-3-12b-it-bnb-4bit-V2\",\n",
    "   \"annagoncalves2/chatbot-Llama-3.1-8B-unsloth-bnb-4bit-V2\",\n",
    "   \"annagoncalves2/chatbot-Llama-3.2-3B-Instruct-unsloth-bnb-4bit-V2\",\n",
    "   \"annagoncalves2/chatbot-phi-4-unsloth-bnb-4bit-V2\",\n",
    "   \"annagoncalves2/chatbot-Qwen2.5-7B-Instruct-bnb-4bit-V2\",\n",
    "   \"annagoncalves2/chatbot-Qwen2.5-14B-Instruct-unsloth-bnb-4bit-V2\",\n",
    "   \"annagoncalves2/chatbot-zephyr-sft-bnb-4bit-V2\",\n",
    "   \"annagoncalves2/chatbot-DeepSeek-R1-Distill-Llama-8B-V2\"\n",
    "]\n",
    "\n",
    "base_dir = \"./models\"\n",
    "\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "for model_id in model_names:\n",
    "    model_dir_name = model_id.split('/')[-1]\n",
    "    local_dir = os.path.join(base_dir, model_dir_name)\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.makedirs(local_dir)\n",
    "        print(f\"Baixando modelo {model_id} para {local_dir} ...\")\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(model_id, token=token, trust_remote_code=True)\n",
    "            config.save_pretrained(local_dir)\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
    "            tokenizer.save_pretrained(local_dir)\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, token=token, trust_remote_code=True)\n",
    "            model.save_pretrained(local_dir)\n",
    "\n",
    "            print(f\"Modelo {model_id} salvo em {local_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao baixar {model_id}: {e}\")\n",
    "    else:\n",
    "        print(f\"Modelo já existe localmente em {local_dir}, pulando download.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abf8ec",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "152d34ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOlá, como você está hoje?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(prompt, max_new_tokens)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# OU, se continuar dando erro, teste movendo inputs para device principal:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# device = next(model.parameters()).device\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2594\u001b[0m     )\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2614\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:3557\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3554\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3557\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    684\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    685\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:422\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `past_key_values` should be either a `Cache` object or `None`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mand\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m DynamicCache()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "local_model_path = \"./models/chatbot-Llama-3.1-8B-unsloth-bnb-4bit-V2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, local_files_only=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def generate_text(prompt, max_new_tokens=50):\n",
    "    # Se der conflito, teste com inputs na CPU (recomendado com device_map=\"auto\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\") \n",
    "\n",
    "    # OU, se continuar dando erro, teste movendo inputs para device principal:\n",
    "    # device = next(model.parameters()).device\n",
    "    # inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"Olá, como você está hoje?\"\n",
    "print(generate_text(prompt, max_new_tokens=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f77b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
